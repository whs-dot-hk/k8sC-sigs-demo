{"version":3,"sources":["webpack:///path---sig-cluster-lifecycle-postmortems-kubeadm-1-6-d3947f838ab7ee275c41.js","webpack:///./.cache/json/sig-cluster-lifecycle-postmortems-kubeadm-1-6.json"],"names":["webpackJsonp","458","module","exports","data","markdownRemark","html","site","siteMetadata","sigs","pathContext","slug"],"mappings":"AAAAA,cAAc,iBAERC,IACA,SAAUC,EAAQC,GCHxBD,EAAAC,SAAkBC,MAAQC,gBAAkBC,KAAA,mndAA4sdC,MAASC,cAAgBC,MAAA,ofAA2fC,aAAgBC,KAAA","file":"path---sig-cluster-lifecycle-postmortems-kubeadm-1-6-d3947f838ab7ee275c41.js","sourcesContent":["webpackJsonp([188761495215006],{\n\n/***/ 458:\n/***/ (function(module, exports) {\n\n\tmodule.exports = {\"data\":{\"markdownRemark\":{\"html\":\"<h1>Kubernetes Postmortem: kubeadm 1.6.0 release</h1>\\n<p><strong>Incident Date:</strong> 2017-03-28</p>\\n<p><strong>Owners:</strong> Jacob Beacham (@pipejakob)</p>\\n<p><strong>Collaborators:</strong> Joe Beda (@jbeda), Mike Danese (@mikedanese), Robert Bailey\\n(@roberthbailey)</p>\\n<p><strong>Status:</strong> [draft | pending feedback | <strong>final</strong>]</p>\\n<p><strong>Summary:</strong> kubeadm 1.6.0 consistently hangs while trying to initialize new\\nclusters. A fix required creating the 1.6.1 patch release six days after 1.6.0.</p>\\n<p><strong>Impact:</strong> Initialization of a new 1.6.0 master using kubeadm.</p>\\n<p><strong>Root Causes:</strong> kubelet’s behavior was changed to report NotReady instead of\\nReady when CNI was unconfigured\\n(<a href=\\\"https://github.com/kubernetes/kubernetes/pull/43474\\\">#43474</a>), which caused\\nkubeadm to hang indefinitely on initialization while waiting for the master node\\nto become Ready (and then schedule a dummy deployment) in order to validate the\\ncontrol plane’s health, which was intended to happen before a CNI provider gets\\nadded.</p>\\n<p><strong>Resolution:</strong> kubeadm initialization now only waits for the master node to\\nregister with the API server, but does not require it to be Ready, and does not\\nattempt a dummy deployment to validate the control plane’s health\\n(<a href=\\\"https://github.com/kubernetes/kubernetes/pull/43835\\\">#43835</a>). This behavior\\nis being revisited for the 1.7 release.</p>\\n<p><strong>Detection:</strong> A customer filed an issue against kubeadm after trying to\\ninitialize a new cluster with the 1.6.0 release\\n(<a href=\\\"https://github.com/kubernetes/kubeadm/issues/212\\\">#212</a>).</p>\\n<p><strong>Lessons Learned:</strong></p>\\n<p><strong>What went well</strong></p>\\n<ul>\\n<li>\\n<p>The bug was discovered quickly after the release.</p>\\n</li>\\n<li>\\n<p>Once the bug was discovered, a solution was ready within a day, and the patch\\nrelease was available five days after that (on a Monday, so the weekend\\naccounted for some of the gap).</p>\\n</li>\\n</ul>\\n<p><strong>What went wrong</strong></p>\\n<ul>\\n<li>\\n<p>The 1.6.0 release of kubeadm never passed end-to-end testing.</p>\\n<ul>\\n<li>\\n<p>End-to-end tests only existed against the master branch instead of the\\nrelease-1.6 branch.</p>\\n</li>\\n<li>\\n<p>Conformance testing of kubeadm requires a functioning CNI provider, but due\\nto changes in 1.6.0 clusters and CNI itself, previous kubeadm-endorsed CNI\\nproviders required updating to reflect the new master taint, RBAC being\\nenabled, the master’s insecure port being disabled, and to tolerate deletion\\nof unknown pods. Without a functional 1.6 CNI provider until very late in\\nthe development cycle, Conformance tests were disabled for kubeadm’s\\nend-to-end jobs in favor of only testing initialization and node joining.</p>\\n</li>\\n<li>\\n<p>The kubeadm end-to-end tests were also constantly breaking throughout the\\ndevelopment cycle due to upstream kubeadm and test-infra changes. There was\\nno automated monitoring to notify the SIG of failures, nor was there a\\nprocess defined for fixing them, which led to a single person manually\\nwatching them and addressing failures as they occurred. As a result, manual\\ntesting was being used near the release milestone, and was successfully\\npassing through the 1.6.0-beta.4 release. Without coordination with the\\nRelease Czar, 1.6.0-rc.1 and 1.6.0 were released without manual end-to-end\\ntesting of kubeadm and contained the regression.</p>\\n</li>\\n</ul>\\n</li>\\n<li>\\n<p>There was a lot of rushing to get the release ready before KubeCon EU, causing\\na shortened timeframe for RC and release, lowered bandwidth for SIG members,\\nand the last SIG meeting before the release to be cancelled, which decreased\\ncommunication.</p>\\n</li>\\n<li>\\n<p>There was no explicit release-readiness sign-off by the SIG. The SIG had\\nchecklists for bringing kubeadm to Beta (the goal for 1.6.0), and they\\nincluded end-to-end tests which were knowingly in a bad state, but no one\\nescalated to delay the release or to remove kubeadm’s Beta status.</p>\\n</li>\\n<li>\\n<p>After the 1.6.0 bug was discovered, there was no public announcement to let\\nusers know about the flaw or the timeline to expect a fix.</p>\\n</li>\\n<li>\\n<p>There were two GitHub issues\\n(<a href=\\\"https://github.com/kubernetes/kubeadm/issues/212\\\">kubeadm#212</a> and\\n<a href=\\\"https://github.com/kubernetes/kubernetes/issues/43815\\\">kubernetes#43815</a>)\\nboth tracking the bug. Both were flooded by users duplicating the bug report\\nor their workarounds, with splintered developer discussions for the short-term\\nand long-term fixes, which made the issue noisy for anyone who just wanted\\nupdates on the status of the official fix. Additional communication occurred\\non Slack channels, so there was no single authoritative source to follow for\\nupdates.</p>\\n</li>\\n<li>\\n<p>Older versions of the kubeadm Debian packages were removed when 1.6.0 was\\nreleased, so users could not fall back on older versions of kubeadm unless\\nthey had cached versions. This was intentional for this release (since prior\\nversions were Alpha and insecure), and shouldn’t happen in future releases,\\nbut it left some users out of luck who were knowingly depending on kubeadm 1.5\\nor wanted to fall back after 1.6.0 failed for them.</p>\\n</li>\\n</ul>\\n<p><strong>Where we got lucky</strong></p>\\n<ul>\\n<li>\\n<p>This bug only manifested during cluster initialization, and occurred\\nconsistently. This meant that it was detected very quickly, was trivial to\\nreproduce, and had minimal impact on customers since they could not have been\\nrelying on the cluster yet. If the bug had been more subtle, it could have\\nbeen triggered at random points during the lifecycle of a cluster, been more\\ndifficult to reproduce and fix, and caused harm to clusters that were already\\nin use by customers.</p>\\n</li>\\n<li>\\n<p>Even without full testing, there were no other kubeadm regressions between\\n1.6.0-rc.1 and 1.6.0.</p>\\n</li>\\n</ul>\\n<p><strong>Action Items:</strong></p>\\n<table>\\n<thead>\\n<tr>\\n<th><strong>Item</strong></th>\\n<th><strong>Type</strong></th>\\n<th><strong>Owner</strong></th>\\n<th><strong>Issue</strong></th>\\n</tr>\\n</thead>\\n<tbody>\\n<tr>\\n<td>Add end-to-end kubeadm postsubmit tests for release-1.6 branch</td>\\n<td>detect</td>\\n<td>pipejakob</td>\\n<td>DONE</td>\\n</tr>\\n<tr>\\n<td>Add end-to-end kubeadm presubmit tests (non-blocking)</td>\\n<td>prevent</td>\\n<td></td>\\n<td><a href=\\\"https://github.com/kubernetes/kubeadm/issues/250\\\">kubeadm#250</a></td>\\n</tr>\\n<tr>\\n<td>Add end-to-end kubeadm variants that use non-third-party CNI providers, like “bridge”</td>\\n<td>prevent</td>\\n<td></td>\\n<td><a href=\\\"https://github.com/kubernetes/kubeadm/issues/218\\\">kubeadm#218</a></td>\\n</tr>\\n<tr>\\n<td>Notify SIG on kubeadm postsubmit end-to-end test failures</td>\\n<td>detect</td>\\n<td></td>\\n<td><a href=\\\"https://github.com/kubernetes/test-infra/issues/2555\\\">test-infra#2555</a></td>\\n</tr>\\n<tr>\\n<td>Define process of who should triage and/or fix kubeadm end-to-end test failures, and how</td>\\n<td>prevent</td>\\n<td></td>\\n<td><a href=\\\"https://github.com/kubernetes/kubeadm/issues/251\\\">kubeadm#251</a></td>\\n</tr>\\n<tr>\\n<td>Do not remove old versions from distribution repositories during release</td>\\n<td>mitigate</td>\\n<td></td>\\n<td><a href=\\\"https://github.com/kubernetes/kubeadm/issues/252\\\">kubeadm#252</a></td>\\n</tr>\\n<tr>\\n<td>Define kubeadm release process that blocks future releases on its completion (e.g. setup end-to-end tests for new release branch, when and how to make the go/no-go decision)</td>\\n<td>prevent</td>\\n<td></td>\\n<td><a href=\\\"https://github.com/kubernetes/kubeadm/issues/252\\\">kubeadm#252</a></td>\\n</tr>\\n<tr>\\n<td>Document incident response process for critically flawed Kubernetes releases, including how to notify the community and track progress to conclusion</td>\\n<td>mitigate</td>\\n<td></td>\\n<td><a href=\\\"https://github.com/kubernetes/community/issues/564\\\">community#564</a></td>\\n</tr>\\n</tbody>\\n</table>\\n<p><strong>Timeline</strong></p>\\n<p>All times are in 24-hour PST8PDT.</p>\\n<p><strong>2017/02/24</strong></p>\\n<blockquote>\\n<p>06:00 fejta changes e2e-runner.sh\\n(<a href=\\\"https://github.com/kubernetes/test-infra/pull/1657\\\">test-infra#1657</a>),\\ninadvertently regresses kubeadm e2e test</p>\\n</blockquote>\\n<p><strong>2017/03/08</strong></p>\\n<blockquote>\\n<p>13:44 pipejakob fixes regression\\n(<a href=\\\"https://github.com/kubernetes/test-infra/pull/2179\\\">test-infra#2179</a>), but\\nthe e2e test is still failing because of recent kubeadm CLI changes</p>\\n</blockquote>\\n<p><strong>2017/03/08</strong></p>\\n<blockquote>\\n<p>13:22 spxtr refactors prow config\\n(<a href=\\\"https://github.com/kubernetes/test-infra/pull/2192\\\">test-infra#2192</a>),\\nwhich later breaks kubeadm e2e job configuration when it gets pushed (this\\ntimestamp is for the merge, but actual activation of config is unknown since\\nit is done manually)</p>\\n</blockquote>\\n<p><strong>2017/03/09</strong></p>\\n<blockquote>\\n<p>21:43 pipejakob merges commit to accommodate recent kubeadm CLI changes to\\nattempt to fix e2e jobs\\n(<a href=\\\"https://github.com/kubernetes/kubernetes-anywhere/pull/352\\\">kubernetes-anywhere#352</a>)</p>\\n</blockquote>\\n<p><strong>2017/03/13</strong></p>\\n<blockquote>\\n<p>11:27 pipejakob temporarily disables kubeadm e2e Conformance testing\\n(<a href=\\\"https://github.com/kubernetes/test-infra/pull/2184\\\">test-infra#2184</a>) to\\nget a better signal; test runs are back to green but only exercise\\ninitializing the cluster and verifying that nodes join correctly</p>\\n<p>12:01 while still trying to fix CNI providers on kubeadm e2e test, pipejakob\\nfinds that even after accounting for expected changes (master taint renaming,\\nRBAC being enabled, unauthenticated access being turned off), CNI providers\\nstill aren’t working\\n(<a href=\\\"https://github.com/kubernetes/kubeadm/issues/190#issuecomment-286209644\\\">kubeadm#190</a>)</p>\\n</blockquote>\\n<p><strong>2017/03/14</strong></p>\\n<blockquote>\\n<p>13:11 pipejakob fixes kubeadm e2e job configuration (which was pushed at some\\npoint after spxtr’s prow configuration refactoring)\\n(<a href=\\\"https://github.com/kubernetes/test-infra/pull/2246\\\">test-infra#2246</a>)</p>\\n</blockquote>\\n<p><strong>2017/03/16</strong></p>\\n<blockquote>\\n<p>11:23 bboreham fixes the weave-net CNI provider\\n(<a href=\\\"https://github.com/weaveworks/weave/pull/2850\\\">weave#2850</a>) to account for\\n“CNI unknown pod deletion” change</p>\\n<p>14:52 krzyzacy migrates kubeadm e2e job to be scenario/json based\\n(<a href=\\\"https://github.com/kubernetes/test-infra/pull/2141\\\">test-infra#2141</a>),\\nwhich breaks the job.</p>\\n<p>Over the next few days, krzyzacy tries to fix the above regression, but the\\njob is ultimately left failing because Conformance testing has been\\nerroneously re-enabled, which is known to be broken due to CNI issues\\n(<a href=\\\"https://github.com/kubernetes/test-infra/pull/2280\\\">test-infra#2280</a>,\\n<a href=\\\"https://github.com/kubernetes/test-infra/pull/2284\\\">test-infra#2284</a>,\\n<a href=\\\"https://github.com/kubernetes/test-infra/pull/2285\\\">test-infra#2285</a>,\\n<a href=\\\"https://github.com/kubernetes/test-infra/pull/2286\\\">test-infra#2286</a>,\\n<a href=\\\"https://github.com/kubernetes/test-infra/pull/2288\\\">test-infra#2288</a>)</p>\\n</blockquote>\\n<p><strong>2017/03/17</strong></p>\\n<blockquote>\\n<p>10:41 enisoc releases\\n<a href=\\\"https://github.com/kubernetes/kubernetes/commit/b202120be3a97e5f8a5e20da51d0b6f5a1eebd31\\\">1.6.0-beta.4</a>.\\nSince e2e tests are broken, pipejakob manually tests cluster initialization\\nlocally (kubeadm still works), as well as the updated weave-net manifest</p>\\n</blockquote>\\n<p><strong>2017/03/22</strong></p>\\n<blockquote>\\n<p>12:35 dcbw merges change to make kubelet report NotReady when CNI is\\nunconfigured\\n(<a href=\\\"https://github.com/kubernetes/kubernetes/pull/43474\\\">kubernetes#43474</a>),\\nbut e2e tests are already failing so no one notices the kubeadm regression</p>\\n</blockquote>\\n<p><strong>2017/03/24</strong></p>\\n<blockquote>\\n<p>12:06 enisoc releases\\n<a href=\\\"https://github.com/kubernetes/kubernetes/commit/8ea07d1fd277de8ab5ea7f281766760bcb7d0fe5\\\">1.6.0-rc.1</a>.\\nThis was the first release to regress kubeadm, but it goes untested.</p>\\n</blockquote>\\n<p><strong>2017/03/28</strong></p>\\n<blockquote>\\n<p>09:23 enisoc releases Kubernetes\\n<a href=\\\"https://github.com/kubernetes/kubernetes/releases/tag/v1.6.0\\\">1.6.0</a>.</p>\\n<p>09:27 pipejakob updates kubeadm e2e job to use weave-net plugin so that\\nConformance testing can be re-enabled\\n(<a href=\\\"https://github.com/kubernetes/test-infra/pull/2347\\\">test-infra#2347</a>), but\\ndue to the subtle <a href=\\\"https://github.com/kubernetes/kubeadm/issues/219\\\">gcloud ssh\\nbug</a>, the job is still\\nbroken after the update, so it masks the new regression in kubeadm init</p>\\n<p>22:40 jimmycuadra reports kubeadm 1.6.0 being broken (<a href=\\\"https://github.com/kubernetes/kubeadm/issues/212\\\">kubeadm issue\\n212</a>)</p>\\n</blockquote>\\n<p><strong>2017/03/29</strong></p>\\n<blockquote>\\n<p>13:04 kensimon opens PR to fix kubeadm master: “Tolerate node network not\\nbeing ready“\\n(<a href=\\\"https://github.com/kubernetes/kubernetes/pull/43824\\\">kubernetes#43824</a>)</p>\\n<p>18:29 mikedanese opens second PR to fix kubeadm master in different way:\\n“don't wait for first kubelet to be ready and drop dummy deploy.”\\n(<a href=\\\"https://github.com/kubernetes/kubernetes/pull/43835\\\">kubernetes#43835</a>)\\npipejakob helps manually test it for QA purposes.</p>\\n<p>18:51 mikedanese opens PR for cherry-pick of above fix to release-1.6 branch\\n(<a href=\\\"https://github.com/kubernetes/kubernetes/pull/43837\\\">kubernetes#43837</a>)</p>\\n</blockquote>\\n<p><strong>2017/03/30</strong></p>\\n<blockquote>\\n<p>16:57 mikedanese’s kubeadm fix\\n(<a href=\\\"https://github.com/kubernetes/kubernetes/pull/43835\\\">kubernetes#43835</a>) is\\nmerged to master (kensimon’s gets discarded)</p>\\n<p>21:57 mikedanese adds new build of .deb to kubernetes-xenial-unstable channel\\nfor users to test (<a href=\\\"https://github.com/kubernetes/kubernetes/issues/43815#issuecomment-290616036\\\">kubernetes issue\\n43815</a>)</p>\\n</blockquote>\\n<p><strong>2017/03/31</strong></p>\\n<blockquote>\\n<p>00:26 mikedanese’s cherry-pick merged to release-1.6 branch\\n(<a href=\\\"https://github.com/kubernetes/kubernetes/pull/43837\\\">kubernetes#43837</a>)</p>\\n<p>11:34 pipejakob merges CI job for release-1.6 branch\\n(<a href=\\\"https://github.com/kubernetes/test-infra/pull/2352\\\">test-infra#2352</a>)</p>\\n<p>16:30 pipejakob merges quick fix\\n(<a href=\\\"https://github.com/kubernetes/test-infra/pull/2380\\\">test-infra#2380</a>) for\\nthe “gcloud ssh issue,” which fixes Conformance testing</p>\\n</blockquote>\\n<p><strong>2017/04/03</strong></p>\\n<blockquote>\\n<p>13:32 enisoc releases Kubernetes\\n<a href=\\\"https://github.com/kubernetes/kubernetes/releases/tag/v1.6.1\\\">1.6.1</a></p>\\n</blockquote>\"},\"site\":{\"siteMetadata\":{\"sigs\":[\"sig-api-machinery\",\"sig-apps\",\"sig-architecture\",\"sig-auth\",\"sig-autoscaling\",\"sig-aws\",\"sig-azure\",\"sig-big-data\",\"sig-cli\",\"sig-cloud-provider\",\"sig-cluster-lifecycle\",\"sig-cluster-ops\",\"sig-contributor-experience\",\"sig-docs\",\"sig-gcp\",\"sig-ibmcloud\",\"sig-instrumentation\",\"sig-multicluster\",\"sig-network\",\"sig-node\",\"sig-openstack\",\"sig-product-management\",\"sig-release\",\"sig-scalability\",\"sig-scheduling\",\"sig-service-catalog\",\"sig-storage\",\"sig-testing\",\"sig-ui\",\"sig-vmware\",\"sig-windows\"]}}},\"pathContext\":{\"slug\":\"/sig-cluster-lifecycle/postmortems/kubeadm-1.6/\"}}\n\n/***/ })\n\n});\n\n\n// WEBPACK FOOTER //\n// path---sig-cluster-lifecycle-postmortems-kubeadm-1-6-d3947f838ab7ee275c41.js","module.exports = {\"data\":{\"markdownRemark\":{\"html\":\"<h1>Kubernetes Postmortem: kubeadm 1.6.0 release</h1>\\n<p><strong>Incident Date:</strong> 2017-03-28</p>\\n<p><strong>Owners:</strong> Jacob Beacham (@pipejakob)</p>\\n<p><strong>Collaborators:</strong> Joe Beda (@jbeda), Mike Danese (@mikedanese), Robert Bailey\\n(@roberthbailey)</p>\\n<p><strong>Status:</strong> [draft | pending feedback | <strong>final</strong>]</p>\\n<p><strong>Summary:</strong> kubeadm 1.6.0 consistently hangs while trying to initialize new\\nclusters. A fix required creating the 1.6.1 patch release six days after 1.6.0.</p>\\n<p><strong>Impact:</strong> Initialization of a new 1.6.0 master using kubeadm.</p>\\n<p><strong>Root Causes:</strong> kubelet’s behavior was changed to report NotReady instead of\\nReady when CNI was unconfigured\\n(<a href=\\\"https://github.com/kubernetes/kubernetes/pull/43474\\\">#43474</a>), which caused\\nkubeadm to hang indefinitely on initialization while waiting for the master node\\nto become Ready (and then schedule a dummy deployment) in order to validate the\\ncontrol plane’s health, which was intended to happen before a CNI provider gets\\nadded.</p>\\n<p><strong>Resolution:</strong> kubeadm initialization now only waits for the master node to\\nregister with the API server, but does not require it to be Ready, and does not\\nattempt a dummy deployment to validate the control plane’s health\\n(<a href=\\\"https://github.com/kubernetes/kubernetes/pull/43835\\\">#43835</a>). This behavior\\nis being revisited for the 1.7 release.</p>\\n<p><strong>Detection:</strong> A customer filed an issue against kubeadm after trying to\\ninitialize a new cluster with the 1.6.0 release\\n(<a href=\\\"https://github.com/kubernetes/kubeadm/issues/212\\\">#212</a>).</p>\\n<p><strong>Lessons Learned:</strong></p>\\n<p><strong>What went well</strong></p>\\n<ul>\\n<li>\\n<p>The bug was discovered quickly after the release.</p>\\n</li>\\n<li>\\n<p>Once the bug was discovered, a solution was ready within a day, and the patch\\nrelease was available five days after that (on a Monday, so the weekend\\naccounted for some of the gap).</p>\\n</li>\\n</ul>\\n<p><strong>What went wrong</strong></p>\\n<ul>\\n<li>\\n<p>The 1.6.0 release of kubeadm never passed end-to-end testing.</p>\\n<ul>\\n<li>\\n<p>End-to-end tests only existed against the master branch instead of the\\nrelease-1.6 branch.</p>\\n</li>\\n<li>\\n<p>Conformance testing of kubeadm requires a functioning CNI provider, but due\\nto changes in 1.6.0 clusters and CNI itself, previous kubeadm-endorsed CNI\\nproviders required updating to reflect the new master taint, RBAC being\\nenabled, the master’s insecure port being disabled, and to tolerate deletion\\nof unknown pods. Without a functional 1.6 CNI provider until very late in\\nthe development cycle, Conformance tests were disabled for kubeadm’s\\nend-to-end jobs in favor of only testing initialization and node joining.</p>\\n</li>\\n<li>\\n<p>The kubeadm end-to-end tests were also constantly breaking throughout the\\ndevelopment cycle due to upstream kubeadm and test-infra changes. There was\\nno automated monitoring to notify the SIG of failures, nor was there a\\nprocess defined for fixing them, which led to a single person manually\\nwatching them and addressing failures as they occurred. As a result, manual\\ntesting was being used near the release milestone, and was successfully\\npassing through the 1.6.0-beta.4 release. Without coordination with the\\nRelease Czar, 1.6.0-rc.1 and 1.6.0 were released without manual end-to-end\\ntesting of kubeadm and contained the regression.</p>\\n</li>\\n</ul>\\n</li>\\n<li>\\n<p>There was a lot of rushing to get the release ready before KubeCon EU, causing\\na shortened timeframe for RC and release, lowered bandwidth for SIG members,\\nand the last SIG meeting before the release to be cancelled, which decreased\\ncommunication.</p>\\n</li>\\n<li>\\n<p>There was no explicit release-readiness sign-off by the SIG. The SIG had\\nchecklists for bringing kubeadm to Beta (the goal for 1.6.0), and they\\nincluded end-to-end tests which were knowingly in a bad state, but no one\\nescalated to delay the release or to remove kubeadm’s Beta status.</p>\\n</li>\\n<li>\\n<p>After the 1.6.0 bug was discovered, there was no public announcement to let\\nusers know about the flaw or the timeline to expect a fix.</p>\\n</li>\\n<li>\\n<p>There were two GitHub issues\\n(<a href=\\\"https://github.com/kubernetes/kubeadm/issues/212\\\">kubeadm#212</a> and\\n<a href=\\\"https://github.com/kubernetes/kubernetes/issues/43815\\\">kubernetes#43815</a>)\\nboth tracking the bug. Both were flooded by users duplicating the bug report\\nor their workarounds, with splintered developer discussions for the short-term\\nand long-term fixes, which made the issue noisy for anyone who just wanted\\nupdates on the status of the official fix. Additional communication occurred\\non Slack channels, so there was no single authoritative source to follow for\\nupdates.</p>\\n</li>\\n<li>\\n<p>Older versions of the kubeadm Debian packages were removed when 1.6.0 was\\nreleased, so users could not fall back on older versions of kubeadm unless\\nthey had cached versions. This was intentional for this release (since prior\\nversions were Alpha and insecure), and shouldn’t happen in future releases,\\nbut it left some users out of luck who were knowingly depending on kubeadm 1.5\\nor wanted to fall back after 1.6.0 failed for them.</p>\\n</li>\\n</ul>\\n<p><strong>Where we got lucky</strong></p>\\n<ul>\\n<li>\\n<p>This bug only manifested during cluster initialization, and occurred\\nconsistently. This meant that it was detected very quickly, was trivial to\\nreproduce, and had minimal impact on customers since they could not have been\\nrelying on the cluster yet. If the bug had been more subtle, it could have\\nbeen triggered at random points during the lifecycle of a cluster, been more\\ndifficult to reproduce and fix, and caused harm to clusters that were already\\nin use by customers.</p>\\n</li>\\n<li>\\n<p>Even without full testing, there were no other kubeadm regressions between\\n1.6.0-rc.1 and 1.6.0.</p>\\n</li>\\n</ul>\\n<p><strong>Action Items:</strong></p>\\n<table>\\n<thead>\\n<tr>\\n<th><strong>Item</strong></th>\\n<th><strong>Type</strong></th>\\n<th><strong>Owner</strong></th>\\n<th><strong>Issue</strong></th>\\n</tr>\\n</thead>\\n<tbody>\\n<tr>\\n<td>Add end-to-end kubeadm postsubmit tests for release-1.6 branch</td>\\n<td>detect</td>\\n<td>pipejakob</td>\\n<td>DONE</td>\\n</tr>\\n<tr>\\n<td>Add end-to-end kubeadm presubmit tests (non-blocking)</td>\\n<td>prevent</td>\\n<td></td>\\n<td><a href=\\\"https://github.com/kubernetes/kubeadm/issues/250\\\">kubeadm#250</a></td>\\n</tr>\\n<tr>\\n<td>Add end-to-end kubeadm variants that use non-third-party CNI providers, like “bridge”</td>\\n<td>prevent</td>\\n<td></td>\\n<td><a href=\\\"https://github.com/kubernetes/kubeadm/issues/218\\\">kubeadm#218</a></td>\\n</tr>\\n<tr>\\n<td>Notify SIG on kubeadm postsubmit end-to-end test failures</td>\\n<td>detect</td>\\n<td></td>\\n<td><a href=\\\"https://github.com/kubernetes/test-infra/issues/2555\\\">test-infra#2555</a></td>\\n</tr>\\n<tr>\\n<td>Define process of who should triage and/or fix kubeadm end-to-end test failures, and how</td>\\n<td>prevent</td>\\n<td></td>\\n<td><a href=\\\"https://github.com/kubernetes/kubeadm/issues/251\\\">kubeadm#251</a></td>\\n</tr>\\n<tr>\\n<td>Do not remove old versions from distribution repositories during release</td>\\n<td>mitigate</td>\\n<td></td>\\n<td><a href=\\\"https://github.com/kubernetes/kubeadm/issues/252\\\">kubeadm#252</a></td>\\n</tr>\\n<tr>\\n<td>Define kubeadm release process that blocks future releases on its completion (e.g. setup end-to-end tests for new release branch, when and how to make the go/no-go decision)</td>\\n<td>prevent</td>\\n<td></td>\\n<td><a href=\\\"https://github.com/kubernetes/kubeadm/issues/252\\\">kubeadm#252</a></td>\\n</tr>\\n<tr>\\n<td>Document incident response process for critically flawed Kubernetes releases, including how to notify the community and track progress to conclusion</td>\\n<td>mitigate</td>\\n<td></td>\\n<td><a href=\\\"https://github.com/kubernetes/community/issues/564\\\">community#564</a></td>\\n</tr>\\n</tbody>\\n</table>\\n<p><strong>Timeline</strong></p>\\n<p>All times are in 24-hour PST8PDT.</p>\\n<p><strong>2017/02/24</strong></p>\\n<blockquote>\\n<p>06:00 fejta changes e2e-runner.sh\\n(<a href=\\\"https://github.com/kubernetes/test-infra/pull/1657\\\">test-infra#1657</a>),\\ninadvertently regresses kubeadm e2e test</p>\\n</blockquote>\\n<p><strong>2017/03/08</strong></p>\\n<blockquote>\\n<p>13:44 pipejakob fixes regression\\n(<a href=\\\"https://github.com/kubernetes/test-infra/pull/2179\\\">test-infra#2179</a>), but\\nthe e2e test is still failing because of recent kubeadm CLI changes</p>\\n</blockquote>\\n<p><strong>2017/03/08</strong></p>\\n<blockquote>\\n<p>13:22 spxtr refactors prow config\\n(<a href=\\\"https://github.com/kubernetes/test-infra/pull/2192\\\">test-infra#2192</a>),\\nwhich later breaks kubeadm e2e job configuration when it gets pushed (this\\ntimestamp is for the merge, but actual activation of config is unknown since\\nit is done manually)</p>\\n</blockquote>\\n<p><strong>2017/03/09</strong></p>\\n<blockquote>\\n<p>21:43 pipejakob merges commit to accommodate recent kubeadm CLI changes to\\nattempt to fix e2e jobs\\n(<a href=\\\"https://github.com/kubernetes/kubernetes-anywhere/pull/352\\\">kubernetes-anywhere#352</a>)</p>\\n</blockquote>\\n<p><strong>2017/03/13</strong></p>\\n<blockquote>\\n<p>11:27 pipejakob temporarily disables kubeadm e2e Conformance testing\\n(<a href=\\\"https://github.com/kubernetes/test-infra/pull/2184\\\">test-infra#2184</a>) to\\nget a better signal; test runs are back to green but only exercise\\ninitializing the cluster and verifying that nodes join correctly</p>\\n<p>12:01 while still trying to fix CNI providers on kubeadm e2e test, pipejakob\\nfinds that even after accounting for expected changes (master taint renaming,\\nRBAC being enabled, unauthenticated access being turned off), CNI providers\\nstill aren’t working\\n(<a href=\\\"https://github.com/kubernetes/kubeadm/issues/190#issuecomment-286209644\\\">kubeadm#190</a>)</p>\\n</blockquote>\\n<p><strong>2017/03/14</strong></p>\\n<blockquote>\\n<p>13:11 pipejakob fixes kubeadm e2e job configuration (which was pushed at some\\npoint after spxtr’s prow configuration refactoring)\\n(<a href=\\\"https://github.com/kubernetes/test-infra/pull/2246\\\">test-infra#2246</a>)</p>\\n</blockquote>\\n<p><strong>2017/03/16</strong></p>\\n<blockquote>\\n<p>11:23 bboreham fixes the weave-net CNI provider\\n(<a href=\\\"https://github.com/weaveworks/weave/pull/2850\\\">weave#2850</a>) to account for\\n“CNI unknown pod deletion” change</p>\\n<p>14:52 krzyzacy migrates kubeadm e2e job to be scenario/json based\\n(<a href=\\\"https://github.com/kubernetes/test-infra/pull/2141\\\">test-infra#2141</a>),\\nwhich breaks the job.</p>\\n<p>Over the next few days, krzyzacy tries to fix the above regression, but the\\njob is ultimately left failing because Conformance testing has been\\nerroneously re-enabled, which is known to be broken due to CNI issues\\n(<a href=\\\"https://github.com/kubernetes/test-infra/pull/2280\\\">test-infra#2280</a>,\\n<a href=\\\"https://github.com/kubernetes/test-infra/pull/2284\\\">test-infra#2284</a>,\\n<a href=\\\"https://github.com/kubernetes/test-infra/pull/2285\\\">test-infra#2285</a>,\\n<a href=\\\"https://github.com/kubernetes/test-infra/pull/2286\\\">test-infra#2286</a>,\\n<a href=\\\"https://github.com/kubernetes/test-infra/pull/2288\\\">test-infra#2288</a>)</p>\\n</blockquote>\\n<p><strong>2017/03/17</strong></p>\\n<blockquote>\\n<p>10:41 enisoc releases\\n<a href=\\\"https://github.com/kubernetes/kubernetes/commit/b202120be3a97e5f8a5e20da51d0b6f5a1eebd31\\\">1.6.0-beta.4</a>.\\nSince e2e tests are broken, pipejakob manually tests cluster initialization\\nlocally (kubeadm still works), as well as the updated weave-net manifest</p>\\n</blockquote>\\n<p><strong>2017/03/22</strong></p>\\n<blockquote>\\n<p>12:35 dcbw merges change to make kubelet report NotReady when CNI is\\nunconfigured\\n(<a href=\\\"https://github.com/kubernetes/kubernetes/pull/43474\\\">kubernetes#43474</a>),\\nbut e2e tests are already failing so no one notices the kubeadm regression</p>\\n</blockquote>\\n<p><strong>2017/03/24</strong></p>\\n<blockquote>\\n<p>12:06 enisoc releases\\n<a href=\\\"https://github.com/kubernetes/kubernetes/commit/8ea07d1fd277de8ab5ea7f281766760bcb7d0fe5\\\">1.6.0-rc.1</a>.\\nThis was the first release to regress kubeadm, but it goes untested.</p>\\n</blockquote>\\n<p><strong>2017/03/28</strong></p>\\n<blockquote>\\n<p>09:23 enisoc releases Kubernetes\\n<a href=\\\"https://github.com/kubernetes/kubernetes/releases/tag/v1.6.0\\\">1.6.0</a>.</p>\\n<p>09:27 pipejakob updates kubeadm e2e job to use weave-net plugin so that\\nConformance testing can be re-enabled\\n(<a href=\\\"https://github.com/kubernetes/test-infra/pull/2347\\\">test-infra#2347</a>), but\\ndue to the subtle <a href=\\\"https://github.com/kubernetes/kubeadm/issues/219\\\">gcloud ssh\\nbug</a>, the job is still\\nbroken after the update, so it masks the new regression in kubeadm init</p>\\n<p>22:40 jimmycuadra reports kubeadm 1.6.0 being broken (<a href=\\\"https://github.com/kubernetes/kubeadm/issues/212\\\">kubeadm issue\\n212</a>)</p>\\n</blockquote>\\n<p><strong>2017/03/29</strong></p>\\n<blockquote>\\n<p>13:04 kensimon opens PR to fix kubeadm master: “Tolerate node network not\\nbeing ready“\\n(<a href=\\\"https://github.com/kubernetes/kubernetes/pull/43824\\\">kubernetes#43824</a>)</p>\\n<p>18:29 mikedanese opens second PR to fix kubeadm master in different way:\\n“don't wait for first kubelet to be ready and drop dummy deploy.”\\n(<a href=\\\"https://github.com/kubernetes/kubernetes/pull/43835\\\">kubernetes#43835</a>)\\npipejakob helps manually test it for QA purposes.</p>\\n<p>18:51 mikedanese opens PR for cherry-pick of above fix to release-1.6 branch\\n(<a href=\\\"https://github.com/kubernetes/kubernetes/pull/43837\\\">kubernetes#43837</a>)</p>\\n</blockquote>\\n<p><strong>2017/03/30</strong></p>\\n<blockquote>\\n<p>16:57 mikedanese’s kubeadm fix\\n(<a href=\\\"https://github.com/kubernetes/kubernetes/pull/43835\\\">kubernetes#43835</a>) is\\nmerged to master (kensimon’s gets discarded)</p>\\n<p>21:57 mikedanese adds new build of .deb to kubernetes-xenial-unstable channel\\nfor users to test (<a href=\\\"https://github.com/kubernetes/kubernetes/issues/43815#issuecomment-290616036\\\">kubernetes issue\\n43815</a>)</p>\\n</blockquote>\\n<p><strong>2017/03/31</strong></p>\\n<blockquote>\\n<p>00:26 mikedanese’s cherry-pick merged to release-1.6 branch\\n(<a href=\\\"https://github.com/kubernetes/kubernetes/pull/43837\\\">kubernetes#43837</a>)</p>\\n<p>11:34 pipejakob merges CI job for release-1.6 branch\\n(<a href=\\\"https://github.com/kubernetes/test-infra/pull/2352\\\">test-infra#2352</a>)</p>\\n<p>16:30 pipejakob merges quick fix\\n(<a href=\\\"https://github.com/kubernetes/test-infra/pull/2380\\\">test-infra#2380</a>) for\\nthe “gcloud ssh issue,” which fixes Conformance testing</p>\\n</blockquote>\\n<p><strong>2017/04/03</strong></p>\\n<blockquote>\\n<p>13:32 enisoc releases Kubernetes\\n<a href=\\\"https://github.com/kubernetes/kubernetes/releases/tag/v1.6.1\\\">1.6.1</a></p>\\n</blockquote>\"},\"site\":{\"siteMetadata\":{\"sigs\":[\"sig-api-machinery\",\"sig-apps\",\"sig-architecture\",\"sig-auth\",\"sig-autoscaling\",\"sig-aws\",\"sig-azure\",\"sig-big-data\",\"sig-cli\",\"sig-cloud-provider\",\"sig-cluster-lifecycle\",\"sig-cluster-ops\",\"sig-contributor-experience\",\"sig-docs\",\"sig-gcp\",\"sig-ibmcloud\",\"sig-instrumentation\",\"sig-multicluster\",\"sig-network\",\"sig-node\",\"sig-openstack\",\"sig-product-management\",\"sig-release\",\"sig-scalability\",\"sig-scheduling\",\"sig-service-catalog\",\"sig-storage\",\"sig-testing\",\"sig-ui\",\"sig-vmware\",\"sig-windows\"]}}},\"pathContext\":{\"slug\":\"/sig-cluster-lifecycle/postmortems/kubeadm-1.6/\"}}\n\n\n//////////////////\n// WEBPACK FOOTER\n// ./~/json-loader!./.cache/json/sig-cluster-lifecycle-postmortems-kubeadm-1-6.json\n// module id = 458\n// module chunks = 188761495215006"],"sourceRoot":""}