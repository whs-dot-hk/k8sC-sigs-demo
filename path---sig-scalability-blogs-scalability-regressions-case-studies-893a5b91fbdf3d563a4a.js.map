{"version":3,"sources":["webpack:///path---sig-scalability-blogs-scalability-regressions-case-studies-893a5b91fbdf3d563a4a.js","webpack:///./.cache/json/sig-scalability-blogs-scalability-regressions-case-studies.json"],"names":["webpackJsonp","483","module","exports","data","markdownRemark","html","site","siteMetadata","sigs","pathContext","slug"],"mappings":"AAAAA,cAAc,iBAERC,IACA,SAAUC,EAAQC,GCHxBD,EAAAC,SAAkBC,MAAQC,gBAAkBC,KAAA,sljBAAs5jBC,MAASC,cAAgBC,MAAA,ofAA2fC,aAAgBC,KAAA","file":"path---sig-scalability-blogs-scalability-regressions-case-studies-893a5b91fbdf3d563a4a.js","sourcesContent":["webpackJsonp([224483981937263],{\n\n/***/ 483:\n/***/ (function(module, exports) {\n\n\tmodule.exports = {\"data\":{\"markdownRemark\":{\"html\":\"<h1>Kubernetes Scalability/Performance Regressions - Case Studies &#x26; Insights</h1>\\n<p><em>by Shyam JVS, Google Inc</em></p>\\n<p><strong>February 2018</strong></p>\\n<h2>Overview</h2>\\n<p>This document is a compilation of some interesting scalability/performance regression stories from the past. These were identified/studied/fixed largely by sig-scalability. We begin by listing them down, along with their succinct explanations, features/components that were involved, and relevant SIGs (besides sig-scalability). We also accompany them with data on what was the smallest scale, both for real and simulated (i.e kubemark) clusters, that managed to catch those regressions. At the end of the document we draw some useful insights based on the case studies.</p>\\n<h2>Case Studies</h2>\\n<table>\\n<thead>\\n<tr>\\n<th align=\\\"left\\\">Issue</th>\\n<th align=\\\"left\\\">Brief Description</th>\\n<th align=\\\"left\\\">Details</th>\\n<th align=\\\"left\\\">Relevant feature(s)/components(s)</th>\\n<th align=\\\"left\\\">Relevant SIG(s)</th>\\n<th align=\\\"left\\\">Smallest real cluster affected</th>\\n<th align=\\\"left\\\">Smallest kubemark cluster affected</th>\\n</tr>\\n</thead>\\n<tbody>\\n<tr>\\n<td align=\\\"left\\\"><a href=\\\"https://github.com/kubernetes/kubernetes/issues/60035\\\">#60035</a></td>\\n<td align=\\\"left\\\">Kubemark-scale fails with couple of hollow-nodes getting pre-empted due to higher mem usage</td>\\n<td align=\\\"left\\\">Few hollow-nodes started getting pre-empted by kubelets due to memory shortage for running critical pods. The increase in memory usage of hollow-nodes (more specifically hollow kube-proxy) was due to resolving a recent bug with endpoints in kubemark (#59823).</td>\\n<td align=\\\"left\\\"><ul>\\n<li>\\nPre-emption (feature)\\n</li>\\n<li>\\nKubelet\\n</li>\\n<li>\\nKube-proxy mock\\n</li>\\n</ul></td>\\n<td align=\\\"left\\\">-</td>\\n<td align=\\\"left\\\">-</td>\\n<td align=\\\"left\\\">5000</td>\\n</tr>\\n<tr>\\n<td align=\\\"left\\\"><a href=\\\"https://github.com/kubernetes/kubernetes/issues/59823\\\">#59823</a></td>\\n<td align=\\\"left\\\">Endpoints objects in kubemark are empty, leading to misleading performance results</td>\\n<td align=\\\"left\\\">Endpoints objects weren't getting populated with more than a single entry, due to conflicting node names for same pod IP. The reason for pod IPs being the same is a bug with our mock docker-client, which assigned a constant IP to all fake pods. This is probably a regression that didn't exist about an year back. It had significant performance implications (see the bug).</td>\\n<td align=\\\"left\\\"><ul>\\n<li>\\nKubelet mock\\n</li>\\n<li>\\nDocker-client mock\\n</li>\\n<li>\\nKube-proxy mock\\n</li>\\n<li>\\nEndpoints-controller\\n</li>\\n<li>\\nApiserver\\n</li>\\n<li>\\nEtcd\\n</li>\\n</ul></td>\\n<td align=\\\"left\\\">sig-network</td>\\n<td align=\\\"left\\\">-</td>\\n<td align=\\\"left\\\">100</td>\\n</tr>\\n<tr>\\n<td align=\\\"left\\\"><a href=\\\"https://github.com/kubernetes/kubernetes/issues/56061\\\">#56061</a></td>\\n<td align=\\\"left\\\">Apiserver memory usage increased by 10-20% after addition of admission metrics</td>\\n<td align=\\\"left\\\">A bunch of admission metrics were added to the apiserver for monitoring admission plugins, webhooks, etc. Soon after that change we started seeing a 100-200MB increase in memory usage of apiserver on a 100-node cluster. Thanks to the resource usage checks in our performance tests, we were able to spot the regression. It was fixed later by making those metrics lighter (i.e removing some SummaryVec metrics, reducing histogram buckets)</td>\\n<td align=\\\"left\\\"><ul>\\n<li>\\nAdmission control (feature)\\n</li>\\n<li>\\nApiserver\\n</li>\\n<li>\\nPrometheus\\n</li>\\n</ul></td>\\n<td align=\\\"left\\\">sig-api-machinery \\n<br>\\n sig-instrumentation</td>\\n<td align=\\\"left\\\">100</td>\\n<td align=\\\"left\\\">-</td>\\n</tr>\\n<tr>\\n<td align=\\\"left\\\"><a href=\\\"https://github.com/kubernetes/kubernetes/issues/55695\\\">#55695</a></td>\\n<td align=\\\"left\\\">Metadata-proxy not able to handle too many pods per node</td>\\n<td align=\\\"left\\\">Metadata-proxy, a newly enabled node agent for proxy'ing metadata requests coming from pods on the node, was unable to handle load from >70 pods due to memory starvation. This violated our official k8s support for 110 pods/node.</td>\\n<td align=\\\"left\\\"><ul>\\n<li>\\nMetadata concealment (feature)\\n</li>\\n<li>\\nMetadata-proxy agent\\n</li>\\n</ul></td>\\n<td align=\\\"left\\\">sig-auth \\n<br>\\n sig-node</td>\\n<td align=\\\"left\\\">-</td>\\n<td align=\\\"left\\\">500</td>\\n</tr>\\n<tr>\\n<td align=\\\"left\\\"><a href=\\\"https://github.com/kubernetes/kubernetes/issues/55060\\\">#55060</a></td>\\n<td align=\\\"left\\\">Increase in pod startup latency due to Duplicate Address Detection in CNI plugin</td>\\n<td align=\\\"left\\\">An update in the Container Network Interface (CNI) library introduced a new step for DAD, that caused a delay for the CNI plugins waiting on it to finish. Since this was along the code path for container creation, it led to increase in pod startup latency on the kubelet side by more than a second. As a result, we saw violation of our 5s pod-startup latency SLO on reasonably large clusters (where we were already close enough to the SLO earlier).</td>\\n<td align=\\\"left\\\"><ul>\\n<li>\\nContainer networking (feature)\\n</li>\\n<li>\\nKubelet\\n</li>\\n</ul></td>\\n<td align=\\\"left\\\">sig-node \\n<br>\\n sig-network</td>\\n<td align=\\\"left\\\">2000 (though some effect was also seen at 100)</td>\\n<td align=\\\"left\\\">-</td>\\n</tr>\\n<tr>\\n<td align=\\\"left\\\"><a href=\\\"https://github.com/kubernetes/kubernetes/issues/54164\\\">#54164</a></td>\\n<td align=\\\"left\\\">Kube-dns pods coming up super slowly in large clusters due to inter-pod anti-affinity</td>\\n<td align=\\\"left\\\">Kube-dns, a default deployment for k8s clusters, introduced node-level soft inter-pod anti-affinity in order to spread those pods across different nodes. However, the O(pods^2) implementation of the anti-affinity in the scheduler, made their scheduling super-slow. As a result, cluster creation was failing with timeout.</td>\\n<td align=\\\"left\\\"><ul>\\n<li>\\nInter-pod anti-affinity (feature)\\n</li>\\n<li>\\nScheduler\\n</li>\\n<li>\\nKube-dns\\n</li>\\n</ul></td>\\n<td align=\\\"left\\\">sig-scheduling \\n<br>\\n sig-network</td>\\n<td align=\\\"left\\\">2000</td>\\n<td align=\\\"left\\\">-</td>\\n</tr>\\n<tr>\\n<td align=\\\"left\\\"><a href=\\\"https://github.com/kubernetes/kubernetes/issues/53327#issuecomment-336150529\\\">#53327 (part)</a></td>\\n<td align=\\\"left\\\">Performance tests seeing a huge drop in scheduler throughput due to one predicate slowing down</td>\\n<td align=\\\"left\\\">One of the scheduler predicates was changed to compute a random 32-length string. That made the predicate super-slow as it started starving for randomness (especially with the predicate running for each of 1000s of pods) and hugely reduced the scheduler throughput (by ~10 times). After few optimizations to the random pkg (eventually getting rid of the rand() call), this was fixed.</td>\\n<td align=\\\"left\\\"><ul>\\n<li>\\nScheduler\\n</li>\\n</ul></td>\\n<td align=\\\"left\\\">sig-scheduling</td>\\n<td align=\\\"left\\\">100 (mild signal)</td>\\n<td align=\\\"left\\\">500 (strong signal)</td>\\n</tr>\\n<tr>\\n<td align=\\\"left\\\"><a href=\\\"https://github.com/kubernetes/kubernetes/issues/53327#issuecomment-339064229\\\">#53327 (part)</a></td>\\n<td align=\\\"left\\\">Kubemark performance tests fail with timeout during pod deletion due to bug in kubelet mock</td>\\n<td align=\\\"left\\\">The kubelet mock (hollow-kubelet) started showing behavioral difference from the real kubelet due to some changes in the latter. As a result, the hollow-kubelet was failing to delete pods forever under a corner condition, which is - a \\\"DELETE pod\\\" event is received for a pod while kubelet is in the middle of it's container creation. A tricky regression needing quite some hunting before we could set the mock right.</td>\\n<td align=\\\"left\\\"><ul>\\n<li>\\nKubelet\\n</li>\\n<li>\\nKubelet mock\\n</li>\\n</ul></td>\\n<td align=\\\"left\\\">sig-node</td>\\n<td align=\\\"left\\\">-</td>\\n<td align=\\\"left\\\">5000 (also 500, but flakily)</td>\\n</tr>\\n<tr>\\n<td align=\\\"left\\\"><a href=\\\"https://github.com/kubernetes/kubernetes/issues/52284\\\">#52284</a></td>\\n<td align=\\\"left\\\">CIDR allocation super slow with IP aliases</td>\\n<td align=\\\"left\\\">This was a performance issue existing from before, but got exposed as a regression when we turned on IP aliasing for large clusters. CIDR-allocator (part of controller-manager) was having poor performance due to bad design. The main reasons being lack of concurrency and synchronous processing of events from shared informers. A bunch of optimizations later (#52292) fixed it's performance.</td>\\n<td align=\\\"left\\\"><ul>\\n<li>\\nIP-aliasing (feature)\\n</li>\\n<li>\\nController-manager (cidr-allocator)\\n</li>\\n</ul></td>\\n<td align=\\\"left\\\">sig-network</td>\\n<td align=\\\"left\\\">2000</td>\\n<td align=\\\"left\\\">-</td>\\n</tr>\\n<tr>\\n<td align=\\\"left\\\"><a href=\\\"https://github.com/kubernetes/kubernetes/issues/51903\\\">#51903</a></td>\\n<td align=\\\"left\\\">Few nodes failing to start in kubemark due to reduced PIDs limit for docker in newer COS image</td>\\n<td align=\\\"left\\\">When COS m60 image was introduced, we started seeing that some of the kubemark hollow-node pods were failing to start due to docker on the host-node crossing the PID limit. This is a risky regression in terms of the damage it could've caused if rolled out to production, and our scalability tests caught it. Besides the low PID threshold issue, it helped also catch another issue on containerd-shim starting too many threads.</td>\\n<td align=\\\"left\\\"><ul>\\n<li>\\nKubelet\\n</li>\\n<li>\\nDocker\\n</li>\\n<li>\\nContainerd-shim\\n</li>\\n</ul></td>\\n<td align=\\\"left\\\">sig-node</td>\\n<td align=\\\"left\\\">-</td>\\n<td align=\\\"left\\\">500</td>\\n</tr>\\n<tr>\\n<td align=\\\"left\\\"><a href=\\\"https://github.com/kubernetes/kubernetes/issues/51899#issuecomment-331924016\\\">#51899 (part)</a></td>\\n<td align=\\\"left\\\">\\\"PATCH node-status\\\" calls seeing high latency due to blocking on audit-logging</td>\\n<td align=\\\"left\\\">Those calls are made by kubelets once every X seconds - which adds up to be quite some qps for large clusters. Part of handling those calls is audit-logging them. When a change moving the default audit-log format to JSON was made, a performance issue with the design was exposed. The update handler for those calls was doing the audit-writing synchronously (instead of buffering + asynchronous writing), which slowed down those calls by an order of magnitude.</td>\\n<td align=\\\"left\\\"><ul>\\n<li>\\nAudit-logging (feature)\\n</li>\\n<li>\\nApiserver\\n</li>\\n</ul></td>\\n<td align=\\\"left\\\">sig-auth \\n<br>\\n sig-instrumentation \\n<br>\\n sig-api-machinery</td>\\n<td align=\\\"left\\\">2000</td>\\n<td align=\\\"left\\\">-</td>\\n</tr>\\n<tr>\\n<td align=\\\"left\\\"><a href=\\\"https://github.com/kubernetes/kubernetes/issues/51899\\\">#51899 (part)</a></td>\\n<td align=\\\"left\\\">\\\"DELETE pods\\\" API call latencies shot up on large cluster tests due to kubelet thundering herd</td>\\n<td align=\\\"left\\\">A change to kubelet pod deletion resulted in delete pod api calls from kubelets being concentrated immediately after container garbage collection. When performing deletion of large numbers (O(10k)) of pods across large numbers (O(1k)) of nodes, the resulting concentrated delete calls from the kubelets cause increased latency of \\\"DELETE pods\\\" API calls (above our target SLO of 1s).</td>\\n<td align=\\\"left\\\"><ul>\\n<li>\\nContainer GC (feature)\\n</li>\\n<li>\\nKubelet\\n</li>\\n</ul></td>\\n<td align=\\\"left\\\">sig-node</td>\\n<td align=\\\"left\\\">2000</td>\\n<td align=\\\"left\\\">-</td>\\n</tr>\\n<tr>\\n<td align=\\\"left\\\"><a href=\\\"https://github.com/kubernetes/kubernetes/issues/51099\\\">#51099</a></td>\\n<td align=\\\"left\\\">gRPC update causing failure of API calls with large responses</td>\\n<td align=\\\"left\\\">When gRPC vendor library was updated to v1.5.1, the default MTU for response size b/w apiserver &#x3C;-> etcd changed to 4MB. This could only be caught by scalability tests, as our regular tests run at a much smaller scale - so they don't actually encounter such large response sizes.</td>\\n<td align=\\\"left\\\"><ul>\\n<li>\\ngRPC framework (feature)\\n</li>\\n<li>\\nEtcd\\n</li>\\n<li>\\nApiserver\\n</li>\\n</ul></td>\\n<td align=\\\"left\\\">sig-api-machinery</td>\\n<td align=\\\"left\\\">100</td>\\n<td align=\\\"left\\\">100</td>\\n</tr>\\n<tr>\\n<td align=\\\"left\\\"><a href=\\\"https://github.com/kubernetes/kubernetes/issues/50854\\\">#50854</a></td>\\n<td align=\\\"left\\\">Route-controller timing out while listing routes from cloud-provider</td>\\n<td align=\\\"left\\\">Route-controller was failing to list routes from the cloud-provider API and in turn failed to create routes for the nodes. The reason was that the project in which the cluster was being created, started to have another huge cluster running there (with O(5k) routes) which was interfering with the list routes call for this cluster, due to cloud-provider side issues.</td>\\n<td align=\\\"left\\\"><ul>\\n<li>\\nController-manager (route-controller)\\n</li>\\n<li>\\nCloud-provider API (GCE)\\n</li>\\n</ul></td>\\n<td align=\\\"left\\\">sig-network \\n<br>\\n sig-gcp</td>\\n<td align=\\\"left\\\">-</td>\\n<td align=\\\"left\\\">5000 (running besides a real 5000 cluster)</td>\\n</tr>\\n<tr>\\n<td align=\\\"left\\\"><a href=\\\"https://github.com/kubernetes/kubernetes/issues/50366\\\">#50366</a></td>\\n<td align=\\\"left\\\">Failing to fit some pods on cluster due to accidentally increased fluentd resource request</td>\\n<td align=\\\"left\\\">Some change around setting fluentd resource requests accidentally doubled it's CPU request. This was caught by our kubemark scalability test where we tightly fit our hollow-node pods onto a small set of nodes. With the fluentd increase, some of those pods couldn't be scheduled due to CPU shortage and we caught it. This bug was risky for production, as it could've preempted some of the users pods for fluentd (a critical pod).</td>\\n<td align=\\\"left\\\"><ul>\\n<li>\\nResource requests (feature)\\n</li>\\n<li>\\nFluentd\\n</li>\\n</ul></td>\\n<td align=\\\"left\\\">sig-instrumentation</td>\\n<td align=\\\"left\\\">-</td>\\n<td align=\\\"left\\\">500</td>\\n</tr>\\n<tr>\\n<td align=\\\"left\\\"><a href=\\\"https://github.com/kubernetes/kubernetes/issues/48700\\\">#48700</a></td>\\n<td align=\\\"left\\\">Apiserver panic while logging a request in TooManyRequests handler</td>\\n<td align=\\\"left\\\">A change in the ordering of apiserver request handlers (where one of them is the TooManyRequests handler) caused a panic while instrumenting the request. Though this is not a scalability regression per se, this is a scenario which was exposed only by our scale tests where we actually see 429s (TooManyRequests) due to the scale at which we run the clusters (unlike normal scale tests).</td>\\n<td align=\\\"left\\\"><ul>\\n<li>\\nApiserver\\n</li>\\n</ul></td>\\n<td align=\\\"left\\\">sig-api-machinery</td>\\n<td align=\\\"left\\\">100</td>\\n<td align=\\\"left\\\">500</td>\\n</tr>\\n<tr>\\n<td align=\\\"left\\\"><a href=\\\"https://github.com/kubernetes/kubernetes/issues/47419\\\">#47419</a></td>\\n<td align=\\\"left\\\">Performance tests failing due to newly exposed high LIST api latencies</td>\\n<td align=\\\"left\\\">After fixing a notorious bug in the instrumentation code for the 'API request latency' metric, we started seeing performance test failures due to high LIST call latencies. Though it seemed like a regression at first, it was actually a hidden performance issue that was brought to light by the fix. We then realized that list calls were not actually satisfying our 1s api latency SLO and tuned it for them appropriately.</td>\\n<td align=\\\"left\\\"><ul>\\n<li>\\nApiserver\\n</li>\\n</ul></td>\\n<td align=\\\"left\\\">sig-api-machinery \\n<br>\\n sig-instrumentation</td>\\n<td align=\\\"left\\\">2000</td>\\n<td align=\\\"left\\\">5000</td>\\n</tr>\\n<tr>\\n<td align=\\\"left\\\"><a href=\\\"https://github.com/kubernetes/kubernetes/issues/45216\\\">#45216</a></td>\\n<td align=\\\"left\\\">Upgrade to Go 1.8. resulted in significant performance regression</td>\\n<td align=\\\"left\\\">When k8s was upgraded to go-1.8, we were seeing timeouts in our kubemark-scale tests due to ~2x increase in the time taken to create services. After some experimenting/profiling, it seemed to originate from changes to the net/http.(*http2serverConn).serve library function which had some extra cases added to a select statement. One of them added some logic for gracefulShutdown which slowed down the function a lot. It was eventually fixed in a patch release by the golang team.</td>\\n<td align=\\\"left\\\"><ul>\\n<li>\\nGolang (net/http library)\\n</li>\\n</ul></td>\\n<td align=\\\"left\\\">-</td>\\n<td align=\\\"left\\\">-</td>\\n<td align=\\\"left\\\">5000</td>\\n</tr>\\n<tr>\\n<td align=\\\"left\\\"><a href=\\\"https://github.com/kubernetes/kubernetes/issues/42000\\\">#42000</a></td>\\n<td align=\\\"left\\\">Kube-proxy backlog processing causing CPU starvation for kubelet to start new pods</td>\\n<td align=\\\"left\\\">Kube-proxies were slow in processing endpoints updates. As a result, they were building up a backlog of work to be done while load test (which creates many services) was running. Later when the density test ran (where we create 1000s of pods), the kube-proxies were still busy processing the backlog from load test and hence consuming high memory. This memory-starved the kubelets from creating the density pods after cgroups were enabled. Before cgroups, this issue was hidden.</td>\\n<td align=\\\"left\\\"><ul>\\n<li>\\nCgroups (feature)\\n</li>\\n<li>\\nKubelet\\n</li>\\n<li>\\nKube-proxy\\n</li>\\n</ul></td>\\n<td align=\\\"left\\\">sig-network \\n<br>\\n sig-node</td>\\n<td align=\\\"left\\\">-</td>\\n<td align=\\\"left\\\">500</td>\\n</tr>\\n</tbody>\\n</table>\\n<h2>Insights</h2>\\n<ul>\\n<li>On many occasions our scalability tests caught critical/risky bugs which were missed by most other tests. If not caught, those could've seriously jeopardized production-readiness of k8s.</li>\\n<li>SIG-Scalability has caught/fixed several important issues that span across various components, features and SIGs.</li>\\n<li>Around 60% of times (possibly even more), we catch scalability regressions with just our medium-scale (and fast) tests, i.e gce-100 and kubemark-500. Making them run as presubmits should act as a strong shield against regressions.</li>\\n<li>Majority of the remaining ones are caught by our large-scale (and slow) tests, i.e kubemark-5k and gce-2k. Making them as post-submit blockers (given they're \\\"usually\\\" quite healthy) should act as a second layer of protection against regressions.</li>\\n</ul>\"},\"site\":{\"siteMetadata\":{\"sigs\":[\"sig-api-machinery\",\"sig-apps\",\"sig-architecture\",\"sig-auth\",\"sig-autoscaling\",\"sig-aws\",\"sig-azure\",\"sig-big-data\",\"sig-cli\",\"sig-cloud-provider\",\"sig-cluster-lifecycle\",\"sig-cluster-ops\",\"sig-contributor-experience\",\"sig-docs\",\"sig-gcp\",\"sig-ibmcloud\",\"sig-instrumentation\",\"sig-multicluster\",\"sig-network\",\"sig-node\",\"sig-openstack\",\"sig-product-management\",\"sig-release\",\"sig-scalability\",\"sig-scheduling\",\"sig-service-catalog\",\"sig-storage\",\"sig-testing\",\"sig-ui\",\"sig-vmware\",\"sig-windows\"]}}},\"pathContext\":{\"slug\":\"/sig-scalability/blogs/scalability-regressions-case-studies/\"}}\n\n/***/ })\n\n});\n\n\n// WEBPACK FOOTER //\n// path---sig-scalability-blogs-scalability-regressions-case-studies-893a5b91fbdf3d563a4a.js","module.exports = {\"data\":{\"markdownRemark\":{\"html\":\"<h1>Kubernetes Scalability/Performance Regressions - Case Studies &#x26; Insights</h1>\\n<p><em>by Shyam JVS, Google Inc</em></p>\\n<p><strong>February 2018</strong></p>\\n<h2>Overview</h2>\\n<p>This document is a compilation of some interesting scalability/performance regression stories from the past. These were identified/studied/fixed largely by sig-scalability. We begin by listing them down, along with their succinct explanations, features/components that were involved, and relevant SIGs (besides sig-scalability). We also accompany them with data on what was the smallest scale, both for real and simulated (i.e kubemark) clusters, that managed to catch those regressions. At the end of the document we draw some useful insights based on the case studies.</p>\\n<h2>Case Studies</h2>\\n<table>\\n<thead>\\n<tr>\\n<th align=\\\"left\\\">Issue</th>\\n<th align=\\\"left\\\">Brief Description</th>\\n<th align=\\\"left\\\">Details</th>\\n<th align=\\\"left\\\">Relevant feature(s)/components(s)</th>\\n<th align=\\\"left\\\">Relevant SIG(s)</th>\\n<th align=\\\"left\\\">Smallest real cluster affected</th>\\n<th align=\\\"left\\\">Smallest kubemark cluster affected</th>\\n</tr>\\n</thead>\\n<tbody>\\n<tr>\\n<td align=\\\"left\\\"><a href=\\\"https://github.com/kubernetes/kubernetes/issues/60035\\\">#60035</a></td>\\n<td align=\\\"left\\\">Kubemark-scale fails with couple of hollow-nodes getting pre-empted due to higher mem usage</td>\\n<td align=\\\"left\\\">Few hollow-nodes started getting pre-empted by kubelets due to memory shortage for running critical pods. The increase in memory usage of hollow-nodes (more specifically hollow kube-proxy) was due to resolving a recent bug with endpoints in kubemark (#59823).</td>\\n<td align=\\\"left\\\"><ul>\\n<li>\\nPre-emption (feature)\\n</li>\\n<li>\\nKubelet\\n</li>\\n<li>\\nKube-proxy mock\\n</li>\\n</ul></td>\\n<td align=\\\"left\\\">-</td>\\n<td align=\\\"left\\\">-</td>\\n<td align=\\\"left\\\">5000</td>\\n</tr>\\n<tr>\\n<td align=\\\"left\\\"><a href=\\\"https://github.com/kubernetes/kubernetes/issues/59823\\\">#59823</a></td>\\n<td align=\\\"left\\\">Endpoints objects in kubemark are empty, leading to misleading performance results</td>\\n<td align=\\\"left\\\">Endpoints objects weren't getting populated with more than a single entry, due to conflicting node names for same pod IP. The reason for pod IPs being the same is a bug with our mock docker-client, which assigned a constant IP to all fake pods. This is probably a regression that didn't exist about an year back. It had significant performance implications (see the bug).</td>\\n<td align=\\\"left\\\"><ul>\\n<li>\\nKubelet mock\\n</li>\\n<li>\\nDocker-client mock\\n</li>\\n<li>\\nKube-proxy mock\\n</li>\\n<li>\\nEndpoints-controller\\n</li>\\n<li>\\nApiserver\\n</li>\\n<li>\\nEtcd\\n</li>\\n</ul></td>\\n<td align=\\\"left\\\">sig-network</td>\\n<td align=\\\"left\\\">-</td>\\n<td align=\\\"left\\\">100</td>\\n</tr>\\n<tr>\\n<td align=\\\"left\\\"><a href=\\\"https://github.com/kubernetes/kubernetes/issues/56061\\\">#56061</a></td>\\n<td align=\\\"left\\\">Apiserver memory usage increased by 10-20% after addition of admission metrics</td>\\n<td align=\\\"left\\\">A bunch of admission metrics were added to the apiserver for monitoring admission plugins, webhooks, etc. Soon after that change we started seeing a 100-200MB increase in memory usage of apiserver on a 100-node cluster. Thanks to the resource usage checks in our performance tests, we were able to spot the regression. It was fixed later by making those metrics lighter (i.e removing some SummaryVec metrics, reducing histogram buckets)</td>\\n<td align=\\\"left\\\"><ul>\\n<li>\\nAdmission control (feature)\\n</li>\\n<li>\\nApiserver\\n</li>\\n<li>\\nPrometheus\\n</li>\\n</ul></td>\\n<td align=\\\"left\\\">sig-api-machinery \\n<br>\\n sig-instrumentation</td>\\n<td align=\\\"left\\\">100</td>\\n<td align=\\\"left\\\">-</td>\\n</tr>\\n<tr>\\n<td align=\\\"left\\\"><a href=\\\"https://github.com/kubernetes/kubernetes/issues/55695\\\">#55695</a></td>\\n<td align=\\\"left\\\">Metadata-proxy not able to handle too many pods per node</td>\\n<td align=\\\"left\\\">Metadata-proxy, a newly enabled node agent for proxy'ing metadata requests coming from pods on the node, was unable to handle load from >70 pods due to memory starvation. This violated our official k8s support for 110 pods/node.</td>\\n<td align=\\\"left\\\"><ul>\\n<li>\\nMetadata concealment (feature)\\n</li>\\n<li>\\nMetadata-proxy agent\\n</li>\\n</ul></td>\\n<td align=\\\"left\\\">sig-auth \\n<br>\\n sig-node</td>\\n<td align=\\\"left\\\">-</td>\\n<td align=\\\"left\\\">500</td>\\n</tr>\\n<tr>\\n<td align=\\\"left\\\"><a href=\\\"https://github.com/kubernetes/kubernetes/issues/55060\\\">#55060</a></td>\\n<td align=\\\"left\\\">Increase in pod startup latency due to Duplicate Address Detection in CNI plugin</td>\\n<td align=\\\"left\\\">An update in the Container Network Interface (CNI) library introduced a new step for DAD, that caused a delay for the CNI plugins waiting on it to finish. Since this was along the code path for container creation, it led to increase in pod startup latency on the kubelet side by more than a second. As a result, we saw violation of our 5s pod-startup latency SLO on reasonably large clusters (where we were already close enough to the SLO earlier).</td>\\n<td align=\\\"left\\\"><ul>\\n<li>\\nContainer networking (feature)\\n</li>\\n<li>\\nKubelet\\n</li>\\n</ul></td>\\n<td align=\\\"left\\\">sig-node \\n<br>\\n sig-network</td>\\n<td align=\\\"left\\\">2000 (though some effect was also seen at 100)</td>\\n<td align=\\\"left\\\">-</td>\\n</tr>\\n<tr>\\n<td align=\\\"left\\\"><a href=\\\"https://github.com/kubernetes/kubernetes/issues/54164\\\">#54164</a></td>\\n<td align=\\\"left\\\">Kube-dns pods coming up super slowly in large clusters due to inter-pod anti-affinity</td>\\n<td align=\\\"left\\\">Kube-dns, a default deployment for k8s clusters, introduced node-level soft inter-pod anti-affinity in order to spread those pods across different nodes. However, the O(pods^2) implementation of the anti-affinity in the scheduler, made their scheduling super-slow. As a result, cluster creation was failing with timeout.</td>\\n<td align=\\\"left\\\"><ul>\\n<li>\\nInter-pod anti-affinity (feature)\\n</li>\\n<li>\\nScheduler\\n</li>\\n<li>\\nKube-dns\\n</li>\\n</ul></td>\\n<td align=\\\"left\\\">sig-scheduling \\n<br>\\n sig-network</td>\\n<td align=\\\"left\\\">2000</td>\\n<td align=\\\"left\\\">-</td>\\n</tr>\\n<tr>\\n<td align=\\\"left\\\"><a href=\\\"https://github.com/kubernetes/kubernetes/issues/53327#issuecomment-336150529\\\">#53327 (part)</a></td>\\n<td align=\\\"left\\\">Performance tests seeing a huge drop in scheduler throughput due to one predicate slowing down</td>\\n<td align=\\\"left\\\">One of the scheduler predicates was changed to compute a random 32-length string. That made the predicate super-slow as it started starving for randomness (especially with the predicate running for each of 1000s of pods) and hugely reduced the scheduler throughput (by ~10 times). After few optimizations to the random pkg (eventually getting rid of the rand() call), this was fixed.</td>\\n<td align=\\\"left\\\"><ul>\\n<li>\\nScheduler\\n</li>\\n</ul></td>\\n<td align=\\\"left\\\">sig-scheduling</td>\\n<td align=\\\"left\\\">100 (mild signal)</td>\\n<td align=\\\"left\\\">500 (strong signal)</td>\\n</tr>\\n<tr>\\n<td align=\\\"left\\\"><a href=\\\"https://github.com/kubernetes/kubernetes/issues/53327#issuecomment-339064229\\\">#53327 (part)</a></td>\\n<td align=\\\"left\\\">Kubemark performance tests fail with timeout during pod deletion due to bug in kubelet mock</td>\\n<td align=\\\"left\\\">The kubelet mock (hollow-kubelet) started showing behavioral difference from the real kubelet due to some changes in the latter. As a result, the hollow-kubelet was failing to delete pods forever under a corner condition, which is - a \\\"DELETE pod\\\" event is received for a pod while kubelet is in the middle of it's container creation. A tricky regression needing quite some hunting before we could set the mock right.</td>\\n<td align=\\\"left\\\"><ul>\\n<li>\\nKubelet\\n</li>\\n<li>\\nKubelet mock\\n</li>\\n</ul></td>\\n<td align=\\\"left\\\">sig-node</td>\\n<td align=\\\"left\\\">-</td>\\n<td align=\\\"left\\\">5000 (also 500, but flakily)</td>\\n</tr>\\n<tr>\\n<td align=\\\"left\\\"><a href=\\\"https://github.com/kubernetes/kubernetes/issues/52284\\\">#52284</a></td>\\n<td align=\\\"left\\\">CIDR allocation super slow with IP aliases</td>\\n<td align=\\\"left\\\">This was a performance issue existing from before, but got exposed as a regression when we turned on IP aliasing for large clusters. CIDR-allocator (part of controller-manager) was having poor performance due to bad design. The main reasons being lack of concurrency and synchronous processing of events from shared informers. A bunch of optimizations later (#52292) fixed it's performance.</td>\\n<td align=\\\"left\\\"><ul>\\n<li>\\nIP-aliasing (feature)\\n</li>\\n<li>\\nController-manager (cidr-allocator)\\n</li>\\n</ul></td>\\n<td align=\\\"left\\\">sig-network</td>\\n<td align=\\\"left\\\">2000</td>\\n<td align=\\\"left\\\">-</td>\\n</tr>\\n<tr>\\n<td align=\\\"left\\\"><a href=\\\"https://github.com/kubernetes/kubernetes/issues/51903\\\">#51903</a></td>\\n<td align=\\\"left\\\">Few nodes failing to start in kubemark due to reduced PIDs limit for docker in newer COS image</td>\\n<td align=\\\"left\\\">When COS m60 image was introduced, we started seeing that some of the kubemark hollow-node pods were failing to start due to docker on the host-node crossing the PID limit. This is a risky regression in terms of the damage it could've caused if rolled out to production, and our scalability tests caught it. Besides the low PID threshold issue, it helped also catch another issue on containerd-shim starting too many threads.</td>\\n<td align=\\\"left\\\"><ul>\\n<li>\\nKubelet\\n</li>\\n<li>\\nDocker\\n</li>\\n<li>\\nContainerd-shim\\n</li>\\n</ul></td>\\n<td align=\\\"left\\\">sig-node</td>\\n<td align=\\\"left\\\">-</td>\\n<td align=\\\"left\\\">500</td>\\n</tr>\\n<tr>\\n<td align=\\\"left\\\"><a href=\\\"https://github.com/kubernetes/kubernetes/issues/51899#issuecomment-331924016\\\">#51899 (part)</a></td>\\n<td align=\\\"left\\\">\\\"PATCH node-status\\\" calls seeing high latency due to blocking on audit-logging</td>\\n<td align=\\\"left\\\">Those calls are made by kubelets once every X seconds - which adds up to be quite some qps for large clusters. Part of handling those calls is audit-logging them. When a change moving the default audit-log format to JSON was made, a performance issue with the design was exposed. The update handler for those calls was doing the audit-writing synchronously (instead of buffering + asynchronous writing), which slowed down those calls by an order of magnitude.</td>\\n<td align=\\\"left\\\"><ul>\\n<li>\\nAudit-logging (feature)\\n</li>\\n<li>\\nApiserver\\n</li>\\n</ul></td>\\n<td align=\\\"left\\\">sig-auth \\n<br>\\n sig-instrumentation \\n<br>\\n sig-api-machinery</td>\\n<td align=\\\"left\\\">2000</td>\\n<td align=\\\"left\\\">-</td>\\n</tr>\\n<tr>\\n<td align=\\\"left\\\"><a href=\\\"https://github.com/kubernetes/kubernetes/issues/51899\\\">#51899 (part)</a></td>\\n<td align=\\\"left\\\">\\\"DELETE pods\\\" API call latencies shot up on large cluster tests due to kubelet thundering herd</td>\\n<td align=\\\"left\\\">A change to kubelet pod deletion resulted in delete pod api calls from kubelets being concentrated immediately after container garbage collection. When performing deletion of large numbers (O(10k)) of pods across large numbers (O(1k)) of nodes, the resulting concentrated delete calls from the kubelets cause increased latency of \\\"DELETE pods\\\" API calls (above our target SLO of 1s).</td>\\n<td align=\\\"left\\\"><ul>\\n<li>\\nContainer GC (feature)\\n</li>\\n<li>\\nKubelet\\n</li>\\n</ul></td>\\n<td align=\\\"left\\\">sig-node</td>\\n<td align=\\\"left\\\">2000</td>\\n<td align=\\\"left\\\">-</td>\\n</tr>\\n<tr>\\n<td align=\\\"left\\\"><a href=\\\"https://github.com/kubernetes/kubernetes/issues/51099\\\">#51099</a></td>\\n<td align=\\\"left\\\">gRPC update causing failure of API calls with large responses</td>\\n<td align=\\\"left\\\">When gRPC vendor library was updated to v1.5.1, the default MTU for response size b/w apiserver &#x3C;-> etcd changed to 4MB. This could only be caught by scalability tests, as our regular tests run at a much smaller scale - so they don't actually encounter such large response sizes.</td>\\n<td align=\\\"left\\\"><ul>\\n<li>\\ngRPC framework (feature)\\n</li>\\n<li>\\nEtcd\\n</li>\\n<li>\\nApiserver\\n</li>\\n</ul></td>\\n<td align=\\\"left\\\">sig-api-machinery</td>\\n<td align=\\\"left\\\">100</td>\\n<td align=\\\"left\\\">100</td>\\n</tr>\\n<tr>\\n<td align=\\\"left\\\"><a href=\\\"https://github.com/kubernetes/kubernetes/issues/50854\\\">#50854</a></td>\\n<td align=\\\"left\\\">Route-controller timing out while listing routes from cloud-provider</td>\\n<td align=\\\"left\\\">Route-controller was failing to list routes from the cloud-provider API and in turn failed to create routes for the nodes. The reason was that the project in which the cluster was being created, started to have another huge cluster running there (with O(5k) routes) which was interfering with the list routes call for this cluster, due to cloud-provider side issues.</td>\\n<td align=\\\"left\\\"><ul>\\n<li>\\nController-manager (route-controller)\\n</li>\\n<li>\\nCloud-provider API (GCE)\\n</li>\\n</ul></td>\\n<td align=\\\"left\\\">sig-network \\n<br>\\n sig-gcp</td>\\n<td align=\\\"left\\\">-</td>\\n<td align=\\\"left\\\">5000 (running besides a real 5000 cluster)</td>\\n</tr>\\n<tr>\\n<td align=\\\"left\\\"><a href=\\\"https://github.com/kubernetes/kubernetes/issues/50366\\\">#50366</a></td>\\n<td align=\\\"left\\\">Failing to fit some pods on cluster due to accidentally increased fluentd resource request</td>\\n<td align=\\\"left\\\">Some change around setting fluentd resource requests accidentally doubled it's CPU request. This was caught by our kubemark scalability test where we tightly fit our hollow-node pods onto a small set of nodes. With the fluentd increase, some of those pods couldn't be scheduled due to CPU shortage and we caught it. This bug was risky for production, as it could've preempted some of the users pods for fluentd (a critical pod).</td>\\n<td align=\\\"left\\\"><ul>\\n<li>\\nResource requests (feature)\\n</li>\\n<li>\\nFluentd\\n</li>\\n</ul></td>\\n<td align=\\\"left\\\">sig-instrumentation</td>\\n<td align=\\\"left\\\">-</td>\\n<td align=\\\"left\\\">500</td>\\n</tr>\\n<tr>\\n<td align=\\\"left\\\"><a href=\\\"https://github.com/kubernetes/kubernetes/issues/48700\\\">#48700</a></td>\\n<td align=\\\"left\\\">Apiserver panic while logging a request in TooManyRequests handler</td>\\n<td align=\\\"left\\\">A change in the ordering of apiserver request handlers (where one of them is the TooManyRequests handler) caused a panic while instrumenting the request. Though this is not a scalability regression per se, this is a scenario which was exposed only by our scale tests where we actually see 429s (TooManyRequests) due to the scale at which we run the clusters (unlike normal scale tests).</td>\\n<td align=\\\"left\\\"><ul>\\n<li>\\nApiserver\\n</li>\\n</ul></td>\\n<td align=\\\"left\\\">sig-api-machinery</td>\\n<td align=\\\"left\\\">100</td>\\n<td align=\\\"left\\\">500</td>\\n</tr>\\n<tr>\\n<td align=\\\"left\\\"><a href=\\\"https://github.com/kubernetes/kubernetes/issues/47419\\\">#47419</a></td>\\n<td align=\\\"left\\\">Performance tests failing due to newly exposed high LIST api latencies</td>\\n<td align=\\\"left\\\">After fixing a notorious bug in the instrumentation code for the 'API request latency' metric, we started seeing performance test failures due to high LIST call latencies. Though it seemed like a regression at first, it was actually a hidden performance issue that was brought to light by the fix. We then realized that list calls were not actually satisfying our 1s api latency SLO and tuned it for them appropriately.</td>\\n<td align=\\\"left\\\"><ul>\\n<li>\\nApiserver\\n</li>\\n</ul></td>\\n<td align=\\\"left\\\">sig-api-machinery \\n<br>\\n sig-instrumentation</td>\\n<td align=\\\"left\\\">2000</td>\\n<td align=\\\"left\\\">5000</td>\\n</tr>\\n<tr>\\n<td align=\\\"left\\\"><a href=\\\"https://github.com/kubernetes/kubernetes/issues/45216\\\">#45216</a></td>\\n<td align=\\\"left\\\">Upgrade to Go 1.8. resulted in significant performance regression</td>\\n<td align=\\\"left\\\">When k8s was upgraded to go-1.8, we were seeing timeouts in our kubemark-scale tests due to ~2x increase in the time taken to create services. After some experimenting/profiling, it seemed to originate from changes to the net/http.(*http2serverConn).serve library function which had some extra cases added to a select statement. One of them added some logic for gracefulShutdown which slowed down the function a lot. It was eventually fixed in a patch release by the golang team.</td>\\n<td align=\\\"left\\\"><ul>\\n<li>\\nGolang (net/http library)\\n</li>\\n</ul></td>\\n<td align=\\\"left\\\">-</td>\\n<td align=\\\"left\\\">-</td>\\n<td align=\\\"left\\\">5000</td>\\n</tr>\\n<tr>\\n<td align=\\\"left\\\"><a href=\\\"https://github.com/kubernetes/kubernetes/issues/42000\\\">#42000</a></td>\\n<td align=\\\"left\\\">Kube-proxy backlog processing causing CPU starvation for kubelet to start new pods</td>\\n<td align=\\\"left\\\">Kube-proxies were slow in processing endpoints updates. As a result, they were building up a backlog of work to be done while load test (which creates many services) was running. Later when the density test ran (where we create 1000s of pods), the kube-proxies were still busy processing the backlog from load test and hence consuming high memory. This memory-starved the kubelets from creating the density pods after cgroups were enabled. Before cgroups, this issue was hidden.</td>\\n<td align=\\\"left\\\"><ul>\\n<li>\\nCgroups (feature)\\n</li>\\n<li>\\nKubelet\\n</li>\\n<li>\\nKube-proxy\\n</li>\\n</ul></td>\\n<td align=\\\"left\\\">sig-network \\n<br>\\n sig-node</td>\\n<td align=\\\"left\\\">-</td>\\n<td align=\\\"left\\\">500</td>\\n</tr>\\n</tbody>\\n</table>\\n<h2>Insights</h2>\\n<ul>\\n<li>On many occasions our scalability tests caught critical/risky bugs which were missed by most other tests. If not caught, those could've seriously jeopardized production-readiness of k8s.</li>\\n<li>SIG-Scalability has caught/fixed several important issues that span across various components, features and SIGs.</li>\\n<li>Around 60% of times (possibly even more), we catch scalability regressions with just our medium-scale (and fast) tests, i.e gce-100 and kubemark-500. Making them run as presubmits should act as a strong shield against regressions.</li>\\n<li>Majority of the remaining ones are caught by our large-scale (and slow) tests, i.e kubemark-5k and gce-2k. Making them as post-submit blockers (given they're \\\"usually\\\" quite healthy) should act as a second layer of protection against regressions.</li>\\n</ul>\"},\"site\":{\"siteMetadata\":{\"sigs\":[\"sig-api-machinery\",\"sig-apps\",\"sig-architecture\",\"sig-auth\",\"sig-autoscaling\",\"sig-aws\",\"sig-azure\",\"sig-big-data\",\"sig-cli\",\"sig-cloud-provider\",\"sig-cluster-lifecycle\",\"sig-cluster-ops\",\"sig-contributor-experience\",\"sig-docs\",\"sig-gcp\",\"sig-ibmcloud\",\"sig-instrumentation\",\"sig-multicluster\",\"sig-network\",\"sig-node\",\"sig-openstack\",\"sig-product-management\",\"sig-release\",\"sig-scalability\",\"sig-scheduling\",\"sig-service-catalog\",\"sig-storage\",\"sig-testing\",\"sig-ui\",\"sig-vmware\",\"sig-windows\"]}}},\"pathContext\":{\"slug\":\"/sig-scalability/blogs/scalability-regressions-case-studies/\"}}\n\n\n//////////////////\n// WEBPACK FOOTER\n// ./~/json-loader!./.cache/json/sig-scalability-blogs-scalability-regressions-case-studies.json\n// module id = 483\n// module chunks = 224483981937263"],"sourceRoot":""}