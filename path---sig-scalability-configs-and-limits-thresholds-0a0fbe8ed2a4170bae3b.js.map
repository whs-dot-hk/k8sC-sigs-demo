{"version":3,"sources":["webpack:///path---sig-scalability-configs-and-limits-thresholds-0a0fbe8ed2a4170bae3b.js","webpack:///./.cache/json/sig-scalability-configs-and-limits-thresholds.json"],"names":["webpackJsonp","485","module","exports","data","markdownRemark","html","site","siteMetadata","sigs","pathContext","slug"],"mappings":"AAAAA,cAAc,iBAERC,IACA,SAAUC,EAAQC,GCHxBD,EAAAC,SAAkBC,MAAQC,gBAAkBC,KAAA,s9MAAg+MC,MAASC,cAAgBC,MAAA,ofAA2fC,aAAgBC,KAAA","file":"path---sig-scalability-configs-and-limits-thresholds-0a0fbe8ed2a4170bae3b.js","sourcesContent":["webpackJsonp([201896593416083],{\n\n/***/ 485:\n/***/ (function(module, exports) {\n\n\tmodule.exports = {\"data\":{\"markdownRemark\":{\"html\":\"<h1>Kubernetes Scalability thresholds</h1>\\n<h2>Background</h2>\\n<p>Since 1.6 release Kubernetes officially supports 5000-node clusters. However,\\nthe question is what that actually means. As of early Q3 2017 we are in the\\nprocess of defining set of performance-related SLIs (<a href=\\\"https://en.wikipedia.org/wiki/Service_level_indicator\\\">Service Level Indicators</a>)\\nand SLOs (<a href=\\\"https://en.wikipedia.org/wiki/Service_level_objective\\\">Service Level Objectives</a>).</p>\\n<p>However, no matter what SLIs and SLOs we have, there will always be some users\\ncoming and saying that their cluster is not meeting the SLOs. And in most cases\\nit appears that the reason behind is that we (as developers) have silently\\nassumed something (e.g. there will be no more than 10000 services in the\\ncluster) and users were not aware of that.</p>\\n<p>This document is trying to explicitly summarize limits for the number of objects\\nin the system that we are aware of and state if we will try to relax them in the\\nfuture or not.</p>\\n<h2>Kubernetes thresholds</h2>\\n<p>We start with explicit definition of quantities and thresholds we assume are\\nsatisfied in the cluster. This is followed by an explanation for some of those.\\nImportant notes about the numbers:</p>\\n<ol>\\n<li>In most cases, exceeding these thresholds doesn’t mean that the cluster\\nfails over - it just means that its overall performance degrades.</li>\\n<li><strong>Some thresholds below (e.g. total number of all objects, or total number of\\npods or namespaces) are given for the largest possible cluster. For smaller\\nclusters, the limits are proportionally lower.</strong></li>\\n<li>The thresholds obviously differ between different Kubernetes releases\\n(hopefully each of them is non-decreasing). The numbers we present are for\\nthe current release (Kubernetes 1.7 release).</li>\\n<li>There are a lot of factors that influence the thresholds, e.g. etcd version\\nor storage data format. For each of those we assume the default from the\\nrelease to avoid providing numbers for huge number of combinations of those.</li>\\n<li>The “Head threshold” is representing the status of Kubernetes head. This\\ncolumn should be snapshotted at every release to produce per-release\\nthresholds (and dedicated column for each release should then be added).</li>\\n</ol>\\n<table>\\n<thead>\\n<tr>\\n<th>Quantity</th>\\n<th>Head threshold</th>\\n<th>1.8 release</th>\\n<th>Long term goal</th>\\n</tr>\\n</thead>\\n<tbody>\\n<tr>\\n<td>Total number of all objects</td>\\n<td>250000</td>\\n<td></td>\\n<td>1000000</td>\\n</tr>\\n<tr>\\n<td>Number of nodes</td>\\n<td>5000</td>\\n<td></td>\\n<td>5000</td>\\n</tr>\\n<tr>\\n<td>Number of pods</td>\\n<td>150000</td>\\n<td></td>\\n<td>500000</td>\\n</tr>\\n<tr>\\n<td>Number of pods per node\\n<sup>\\n1\\n</sup></td>\\n<td>110</td>\\n<td></td>\\n<td>500</td>\\n</tr>\\n<tr>\\n<td>Number of pods per core\\n<sup>\\n1\\n</sup></td>\\n<td>10</td>\\n<td></td>\\n<td>10</td>\\n</tr>\\n<tr>\\n<td>Number of namespaces (ns)</td>\\n<td>10000</td>\\n<td></td>\\n<td>100000</td>\\n</tr>\\n<tr>\\n<td>Number of pods per ns</td>\\n<td>15000</td>\\n<td></td>\\n<td>50000</td>\\n</tr>\\n<tr>\\n<td>Number of services</td>\\n<td>10000</td>\\n<td></td>\\n<td>100000</td>\\n</tr>\\n<tr>\\n<td>Number of services per ns</td>\\n<td>5000</td>\\n<td></td>\\n<td>5000</td>\\n</tr>\\n<tr>\\n<td>Number of all services backends</td>\\n<td>TBD</td>\\n<td></td>\\n<td>500000</td>\\n</tr>\\n<tr>\\n<td>Number of backends per service</td>\\n<td>5000</td>\\n<td></td>\\n<td>5000</td>\\n</tr>\\n<tr>\\n<td>Number of deployments per ns</td>\\n<td>20000</td>\\n<td></td>\\n<td>10000</td>\\n</tr>\\n<tr>\\n<td>Number of pods per deployment</td>\\n<td>TBD</td>\\n<td></td>\\n<td>10000</td>\\n</tr>\\n<tr>\\n<td>Number of jobs per ns</td>\\n<td>TBD</td>\\n<td></td>\\n<td>1000</td>\\n</tr>\\n<tr>\\n<td>Number of daemon sets per ns</td>\\n<td>TBD</td>\\n<td></td>\\n<td>100</td>\\n</tr>\\n<tr>\\n<td>Number of stateful sets per ns</td>\\n<td>TBD</td>\\n<td></td>\\n<td>100</td>\\n</tr>\\n<tr>\\n<td>Number of secrets per ns</td>\\n<td>TBD</td>\\n<td></td>\\n<td>TBD</td>\\n</tr>\\n<tr>\\n<td>Number of secrets per pod</td>\\n<td>TBD</td>\\n<td></td>\\n<td>TBD</td>\\n</tr>\\n<tr>\\n<td>Number of config maps per ns</td>\\n<td>TBD</td>\\n<td></td>\\n<td>TBD</td>\\n</tr>\\n<tr>\\n<td>Number of config maps per pod</td>\\n<td>TBD</td>\\n<td></td>\\n<td>TBD</td>\\n</tr>\\n<tr>\\n<td>Number of storageclasses</td>\\n<td>TBD</td>\\n<td></td>\\n<td>TBD</td>\\n</tr>\\n<tr>\\n<td>Number of roles and rolebindings</td>\\n<td>TBD</td>\\n<td></td>\\n<td>TBD</td>\\n</tr>\\n</tbody>\\n</table>\\n<p>There are also thresholds for other types, but for those the numbers depend\\nalso on the environment (bare metal or which cloud provider) the cluster is\\nrunning in. These include:</p>\\n<table>\\n<thead>\\n<tr>\\n<th>Quantity</th>\\n<th>Head threshold</th>\\n<th>1.8 release</th>\\n<th>Long term goal</th>\\n</tr>\\n</thead>\\n<tbody>\\n<tr>\\n<td>Number of ingresses</td>\\n<td>TBD</td>\\n<td></td>\\n<td>TBD</td>\\n</tr>\\n<tr>\\n<td>Number of PersistentVolumes</td>\\n<td>TBD</td>\\n<td></td>\\n<td>TBD</td>\\n</tr>\\n<tr>\\n<td>Number of PersistentVolumeClaims per ns</td>\\n<td>TBD</td>\\n<td></td>\\n<td>TBD</td>\\n</tr>\\n<tr>\\n<td>Number of PersistentVolumeClaims per node</td>\\n<td>TBD</td>\\n<td></td>\\n<td>TBD</td>\\n</tr>\\n</tbody>\\n</table>\\n<p>The rationale for some of those numbers:</p>\\n<ol>\\n<li>Total number of objects <br/>\\nThere is a limitation on the total number of objects on the system, as this\\naffects among others etcd and its resource consumption.</li>\\n<li>Number of nodes <br/>\\nWe believe that having clusters with more than 5000 nodes is not the best\\noption and users should consider splitting into multiple clusters. However,\\nwe may consider bumping the long term goal at some time in the future.</li>\\n<li>Number of services and endpoints <br/>\\nEach service port and each service backend has a corresponding entry in\\niptables. Number of backends of a given service impact the size of the\\n<code>Endpoints</code> objects, which impacts size of data that is being sent all over\\nthe system.</li>\\n<li>Number of objects of a given type per namespace <br/>\\nThis holds for different objects (pods, secrets, deployments, ...). There are\\na number of control loops in the system that need to iterate over all objects\\nin a given namespace as a reaction to some changes in state. Having large\\nnumber of objects of a given type in a single namespace can make those loops\\nexpensive and slow down processing given state changes.</li>\\n</ol>\\n<hr>\\n<p><sup>1</sup> The limit for number of pods on a given node is in fact minimum from the “pod per node” and “pods per core times number of cores of a node”.</p>\"},\"site\":{\"siteMetadata\":{\"sigs\":[\"sig-api-machinery\",\"sig-apps\",\"sig-architecture\",\"sig-auth\",\"sig-autoscaling\",\"sig-aws\",\"sig-azure\",\"sig-big-data\",\"sig-cli\",\"sig-cloud-provider\",\"sig-cluster-lifecycle\",\"sig-cluster-ops\",\"sig-contributor-experience\",\"sig-docs\",\"sig-gcp\",\"sig-ibmcloud\",\"sig-instrumentation\",\"sig-multicluster\",\"sig-network\",\"sig-node\",\"sig-openstack\",\"sig-product-management\",\"sig-release\",\"sig-scalability\",\"sig-scheduling\",\"sig-service-catalog\",\"sig-storage\",\"sig-testing\",\"sig-ui\",\"sig-vmware\",\"sig-windows\"]}}},\"pathContext\":{\"slug\":\"/sig-scalability/configs-and-limits/thresholds/\"}}\n\n/***/ })\n\n});\n\n\n// WEBPACK FOOTER //\n// path---sig-scalability-configs-and-limits-thresholds-0a0fbe8ed2a4170bae3b.js","module.exports = {\"data\":{\"markdownRemark\":{\"html\":\"<h1>Kubernetes Scalability thresholds</h1>\\n<h2>Background</h2>\\n<p>Since 1.6 release Kubernetes officially supports 5000-node clusters. However,\\nthe question is what that actually means. As of early Q3 2017 we are in the\\nprocess of defining set of performance-related SLIs (<a href=\\\"https://en.wikipedia.org/wiki/Service_level_indicator\\\">Service Level Indicators</a>)\\nand SLOs (<a href=\\\"https://en.wikipedia.org/wiki/Service_level_objective\\\">Service Level Objectives</a>).</p>\\n<p>However, no matter what SLIs and SLOs we have, there will always be some users\\ncoming and saying that their cluster is not meeting the SLOs. And in most cases\\nit appears that the reason behind is that we (as developers) have silently\\nassumed something (e.g. there will be no more than 10000 services in the\\ncluster) and users were not aware of that.</p>\\n<p>This document is trying to explicitly summarize limits for the number of objects\\nin the system that we are aware of and state if we will try to relax them in the\\nfuture or not.</p>\\n<h2>Kubernetes thresholds</h2>\\n<p>We start with explicit definition of quantities and thresholds we assume are\\nsatisfied in the cluster. This is followed by an explanation for some of those.\\nImportant notes about the numbers:</p>\\n<ol>\\n<li>In most cases, exceeding these thresholds doesn’t mean that the cluster\\nfails over - it just means that its overall performance degrades.</li>\\n<li><strong>Some thresholds below (e.g. total number of all objects, or total number of\\npods or namespaces) are given for the largest possible cluster. For smaller\\nclusters, the limits are proportionally lower.</strong></li>\\n<li>The thresholds obviously differ between different Kubernetes releases\\n(hopefully each of them is non-decreasing). The numbers we present are for\\nthe current release (Kubernetes 1.7 release).</li>\\n<li>There are a lot of factors that influence the thresholds, e.g. etcd version\\nor storage data format. For each of those we assume the default from the\\nrelease to avoid providing numbers for huge number of combinations of those.</li>\\n<li>The “Head threshold” is representing the status of Kubernetes head. This\\ncolumn should be snapshotted at every release to produce per-release\\nthresholds (and dedicated column for each release should then be added).</li>\\n</ol>\\n<table>\\n<thead>\\n<tr>\\n<th>Quantity</th>\\n<th>Head threshold</th>\\n<th>1.8 release</th>\\n<th>Long term goal</th>\\n</tr>\\n</thead>\\n<tbody>\\n<tr>\\n<td>Total number of all objects</td>\\n<td>250000</td>\\n<td></td>\\n<td>1000000</td>\\n</tr>\\n<tr>\\n<td>Number of nodes</td>\\n<td>5000</td>\\n<td></td>\\n<td>5000</td>\\n</tr>\\n<tr>\\n<td>Number of pods</td>\\n<td>150000</td>\\n<td></td>\\n<td>500000</td>\\n</tr>\\n<tr>\\n<td>Number of pods per node\\n<sup>\\n1\\n</sup></td>\\n<td>110</td>\\n<td></td>\\n<td>500</td>\\n</tr>\\n<tr>\\n<td>Number of pods per core\\n<sup>\\n1\\n</sup></td>\\n<td>10</td>\\n<td></td>\\n<td>10</td>\\n</tr>\\n<tr>\\n<td>Number of namespaces (ns)</td>\\n<td>10000</td>\\n<td></td>\\n<td>100000</td>\\n</tr>\\n<tr>\\n<td>Number of pods per ns</td>\\n<td>15000</td>\\n<td></td>\\n<td>50000</td>\\n</tr>\\n<tr>\\n<td>Number of services</td>\\n<td>10000</td>\\n<td></td>\\n<td>100000</td>\\n</tr>\\n<tr>\\n<td>Number of services per ns</td>\\n<td>5000</td>\\n<td></td>\\n<td>5000</td>\\n</tr>\\n<tr>\\n<td>Number of all services backends</td>\\n<td>TBD</td>\\n<td></td>\\n<td>500000</td>\\n</tr>\\n<tr>\\n<td>Number of backends per service</td>\\n<td>5000</td>\\n<td></td>\\n<td>5000</td>\\n</tr>\\n<tr>\\n<td>Number of deployments per ns</td>\\n<td>20000</td>\\n<td></td>\\n<td>10000</td>\\n</tr>\\n<tr>\\n<td>Number of pods per deployment</td>\\n<td>TBD</td>\\n<td></td>\\n<td>10000</td>\\n</tr>\\n<tr>\\n<td>Number of jobs per ns</td>\\n<td>TBD</td>\\n<td></td>\\n<td>1000</td>\\n</tr>\\n<tr>\\n<td>Number of daemon sets per ns</td>\\n<td>TBD</td>\\n<td></td>\\n<td>100</td>\\n</tr>\\n<tr>\\n<td>Number of stateful sets per ns</td>\\n<td>TBD</td>\\n<td></td>\\n<td>100</td>\\n</tr>\\n<tr>\\n<td>Number of secrets per ns</td>\\n<td>TBD</td>\\n<td></td>\\n<td>TBD</td>\\n</tr>\\n<tr>\\n<td>Number of secrets per pod</td>\\n<td>TBD</td>\\n<td></td>\\n<td>TBD</td>\\n</tr>\\n<tr>\\n<td>Number of config maps per ns</td>\\n<td>TBD</td>\\n<td></td>\\n<td>TBD</td>\\n</tr>\\n<tr>\\n<td>Number of config maps per pod</td>\\n<td>TBD</td>\\n<td></td>\\n<td>TBD</td>\\n</tr>\\n<tr>\\n<td>Number of storageclasses</td>\\n<td>TBD</td>\\n<td></td>\\n<td>TBD</td>\\n</tr>\\n<tr>\\n<td>Number of roles and rolebindings</td>\\n<td>TBD</td>\\n<td></td>\\n<td>TBD</td>\\n</tr>\\n</tbody>\\n</table>\\n<p>There are also thresholds for other types, but for those the numbers depend\\nalso on the environment (bare metal or which cloud provider) the cluster is\\nrunning in. These include:</p>\\n<table>\\n<thead>\\n<tr>\\n<th>Quantity</th>\\n<th>Head threshold</th>\\n<th>1.8 release</th>\\n<th>Long term goal</th>\\n</tr>\\n</thead>\\n<tbody>\\n<tr>\\n<td>Number of ingresses</td>\\n<td>TBD</td>\\n<td></td>\\n<td>TBD</td>\\n</tr>\\n<tr>\\n<td>Number of PersistentVolumes</td>\\n<td>TBD</td>\\n<td></td>\\n<td>TBD</td>\\n</tr>\\n<tr>\\n<td>Number of PersistentVolumeClaims per ns</td>\\n<td>TBD</td>\\n<td></td>\\n<td>TBD</td>\\n</tr>\\n<tr>\\n<td>Number of PersistentVolumeClaims per node</td>\\n<td>TBD</td>\\n<td></td>\\n<td>TBD</td>\\n</tr>\\n</tbody>\\n</table>\\n<p>The rationale for some of those numbers:</p>\\n<ol>\\n<li>Total number of objects <br/>\\nThere is a limitation on the total number of objects on the system, as this\\naffects among others etcd and its resource consumption.</li>\\n<li>Number of nodes <br/>\\nWe believe that having clusters with more than 5000 nodes is not the best\\noption and users should consider splitting into multiple clusters. However,\\nwe may consider bumping the long term goal at some time in the future.</li>\\n<li>Number of services and endpoints <br/>\\nEach service port and each service backend has a corresponding entry in\\niptables. Number of backends of a given service impact the size of the\\n<code>Endpoints</code> objects, which impacts size of data that is being sent all over\\nthe system.</li>\\n<li>Number of objects of a given type per namespace <br/>\\nThis holds for different objects (pods, secrets, deployments, ...). There are\\na number of control loops in the system that need to iterate over all objects\\nin a given namespace as a reaction to some changes in state. Having large\\nnumber of objects of a given type in a single namespace can make those loops\\nexpensive and slow down processing given state changes.</li>\\n</ol>\\n<hr>\\n<p><sup>1</sup> The limit for number of pods on a given node is in fact minimum from the “pod per node” and “pods per core times number of cores of a node”.</p>\"},\"site\":{\"siteMetadata\":{\"sigs\":[\"sig-api-machinery\",\"sig-apps\",\"sig-architecture\",\"sig-auth\",\"sig-autoscaling\",\"sig-aws\",\"sig-azure\",\"sig-big-data\",\"sig-cli\",\"sig-cloud-provider\",\"sig-cluster-lifecycle\",\"sig-cluster-ops\",\"sig-contributor-experience\",\"sig-docs\",\"sig-gcp\",\"sig-ibmcloud\",\"sig-instrumentation\",\"sig-multicluster\",\"sig-network\",\"sig-node\",\"sig-openstack\",\"sig-product-management\",\"sig-release\",\"sig-scalability\",\"sig-scheduling\",\"sig-service-catalog\",\"sig-storage\",\"sig-testing\",\"sig-ui\",\"sig-vmware\",\"sig-windows\"]}}},\"pathContext\":{\"slug\":\"/sig-scalability/configs-and-limits/thresholds/\"}}\n\n\n//////////////////\n// WEBPACK FOOTER\n// ./~/json-loader!./.cache/json/sig-scalability-configs-and-limits-thresholds.json\n// module id = 485\n// module chunks = 201896593416083"],"sourceRoot":""}