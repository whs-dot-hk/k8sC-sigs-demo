webpackJsonp([95979276948665],{497:function(e,t){e.exports={data:{markdownRemark:{html:'<h1>Kubernetes Retrospective: 1.3 Storage Post Code Freeze Submission</h1>\n<p><em>Last updated: September 20, 2016</em></p>\n<p><strong>Owners:</strong> Brad Childs (<a href="https://github.com/childsb">@childsb</a>), Michael Rubin (<a href="https://github.com/matchstick">@matchstick</a>)</p>\n<p><strong>Collaborators:</strong> Saad Ali (<a href="https://github.com/saad-ali">@saad-ali</a>), Paul Morie (<a href="https://github.com/pmorie">@pmorie</a>), Tim Hockins (<a href="https://github.com/thockin">@thockin</a>), Steve Watt (<a href="https://github.com/wattsteve">@wattsteve</a>)</p>\n<p><strong>Links:</strong></p>\n<ul>\n<li><a href="https://git.k8s.io/sig-release/releases/release-1.3/release-1.3.md">1.3 Schedule Dates</a></li>\n</ul>\n<h2>Purpose</h2>\n<p>This document is intended to chronicle the decisions made by the <a href="/k8s-c-sigs-demo/sig-storage/README.md">Storage SIG</a> near the end of the Kubernetes 1.3 release with the storage stack that were not well understood by the wider community. This document should explain those decisions, why the SIG made the exception, detail the impact, and offer lessons learned for the future.</p>\n<h2>What Problem Were We Trying to Solve?</h2>\n<p>Kubernetes 1.2 had numerous problems and issues with the storage framework that arose from organic growth of the architecture as it tackled numerous new features it was not initially designed for. There were race conditions, maintenance and stability issues, and architectural problems with all major components of the storage stack including the Persistent Volume (PV) &#x26; Persistent Volume Claim (PVC) controller and the attach/detach and mount/unmount logic.</p>\n<p>The PV/PVC controller handles the connection of provisioned storage volumes to a user claim for storage. The attach/detach logic handles how volumes are attached to hosts. The mount/unmount logic handles how volumes are mounted into containers. Architecturally in 1.2 the attach/detach logic was part of the kubelet on the node.</p>\n<p>A characteristic list of issues (as not all of them were well captured in GitHub issues) include:</p>\n<ol>\n<li>Approximately a 5% rate of incidents under controlled conditions where operations related to Claims binding to Persistent Volumes would fail.</li>\n<li>Rapid creation and deletion of pods referencing the same volume could result in attach/detach events being triggered out of order resulting in detaching of volumes in use (resulting in data loss/corruption). The current 1.2 work around was to fail the operation. This led to surprises and failures in launching pods that referenced the same volume.</li>\n<li>Item #2 created instability in use of multiple pods referencing the same Volume (a supported feature) even when only one pod uses it at a time (<a href="https://github.com/kubernetes/kubernetes/issues/19953">#19953</a>)</li>\n<li>Hiccups in the operation flow of binding the Claims to Volumes resulted in timeouts of tens of minutes.</li>\n<li>External object bleeding. Much of the logic was centered on a state machine that lived in the kubelet. Other kube components had to be aware of the state machine and other aspects of the binding framework to use Volumes.</li>\n<li>Maintenance was difficult as this work was implemented in three different controllers that spread the logic for provisioning, binding, and recycling Volumes.</li>\n<li>Kubelet failures on the Node could “strand” storage. Requiring users to manually unmount storage.</li>\n<li>A pod\'s long running detach routine could impact other pods as the operations run synchronously in the kubelet sync loop.</li>\n<li>Nodes required elevated privileges to be able to trigger attach/detach. Ideally attach/detach should be triggered from master which is considered more secure (see Issue <a href="https://github.com/kubernetes/kubernetes/issues/12399">#12399</a>).</li>\n</ol>\n<p>Below are the Github Issues that were filed for this area:</p>\n<ul>\n<li>Problem rescheduling POD with GCE PD disk attached (<a href="https://github.com/kubernetes/kubernetes/issues/14642">#14642</a>)</li>\n<li>GCE PD Volumes already attached to a node fail with "Error 400: The disk resource is already being used by" node (<a href="https://github.com/kubernetes/kubernetes/issues/19953">#19953</a>)</li>\n<li>Kubelet should be able to delete 10 pods per node in 1m0s (<a href="https://github.com/kubernetes/kubernetes/issues/23591">#23591</a>)</li>\n<li>Detach EBS volumes when node restarted (<a href="https://github.com/kubernetes/kubernetes/issues/26847">#26847</a>)</li>\n<li>Technical debt: refactor Kubelet.HandlePodCleanups into separate thread (<a href="https://github.com/kubernetes/kubernetes/issues/19645">#19645</a>)</li>\n<li>EBS volume mount failures due to "... already attached to an instance" are not retried (<a href="https://github.com/kubernetes/kubernetes/issues/18785">#18785</a>)</li>\n<li>Node upgrades: e2e test: ensure persistent volumes survive when pods die (<a href="https://github.com/kubernetes/kubernetes/issues/6084">#6084</a>)</li>\n<li>Consider "attach controller" to secure cloud provider credentials (<a href="https://github.com/kubernetes/kubernetes/issues/12399">#12399</a>)</li>\n</ul>\n<h2>How Did We Solve the Problem?</h2>\n<p>Addressing these issues was the main deliverable for storage in 1.3. This required an in depth rewrite of several components.</p>\n<p>Early in the 1.3 development cycle (March 28 to April 1, 2016) several community members in the Storage SIG met at a week long face-to-face summit at Google\'s office in Mountain View to address these issues. A plan was established to approach the attach/detach/mount/unmount issues as a deliberate effort with contributors already handling the design. Since that work was already in flight and a plan established, the majority of the summit was devoted to resolving the PV/PVC controller issues. Meeting notes were captured <a href="/k8s-c-sigs-demo/sig-storage/1.3-retrospective/2016-03-28_Storage-SIG-F2F_Notes.pdf">in this document</a>.</p>\n<p>Three projects were planned to fix the issues outlined above:</p>\n<ul>\n<li>PV/PVC Controller Redesign (a.k.a. Provisioner/Binder/Recycler controller)</li>\n<li>Attach/Detach Controller</li>\n<li>Kubelet Volume Redesign</li>\n</ul>\n<p>At the end of the design summit, the attendees of the summit agreed to pseudo code for a re-written PV/PVC controller and a go-forward plan for the attach/detach controller and kubelet volume redesign.</p>\n<p>Resources were established for the PV/PVC controller rework at the conclusion of the design summit and the existing resources on the attach/detach/mount/unmount work deemed acceptable to complete the other two projects.</p>\n<p>At this point, a group of engineers were assigned to work on the three efforts that compromised the overhaul. The plan was to not only include development work but comprehensive testing with time to have the functionality “soak” weeks before 1.3 shipped. These engineers were composed of a hybrid team of Red Hat and Google. The allocation of work made making all three sub deliverables in 1.3 aggressive but reasonable.</p>\n<p>Near the end of 1.3 development, on May 13, 2016, approximately one week prior to code freeze, a key engineer for this effort left the project. This disrupted the Kubelet Volume Redesign effort. The PV/PVC controller was complete (PR <a href="https://github.com/kubernetes/kubernetes/pull/24331">#24331</a>) and committed at this point. However the Attach/Detach Controller was dependent on the Kubelet Volume Redesign and was impacted.</p>\n<p>The leads involved with the projects met and the Kubelet Volume Redesign work was handed off from one engineer to another familiar with Storage. The decision to continue this work after the 1.3 code freeze date of May 20 was based on the need to address the outstanding issues in 1.2. Also much of the Attach/Detach Controller work had been committed but was dependent on the Kubelet Volume Redesign effort.</p>\n<p>The Kubelet Volume Redesign involved changing fundamental assumptions of data flow and volume operations in kubelet. The high level change introduced a new volume manager in kubelet that handled mount/unmount logic and enabled attach/detach logic to be offloaded to the master (by default, while retaining the ability for kubelet to do attach/detach on its own). The remaining work to complete the effort was the kubelet volume redesign PR (<a href="https://github.com/kubernetes/kubernetes/pull/26801">#26801</a>). This combined with the attach/detach controller (PR <a href="https://github.com/kubernetes/kubernetes/pull/25457">#25457</a>) were substantial changes to the stack.</p>\n<h2>Impact:</h2>\n<ol>\n<li><strong>Release delay</strong></li>\n<li>The large amount of churn so late in the release with little stabilization time resulted in the delay of the release by one week: The Kubernetes 1.3 release <a href="https://git.k8s.io/sig-release/releases/release-1.3/release-1.3.md">was targeted</a> for June 20 to June 24, 2016. It ended up <a href="https://github.com/kubernetes/kubernetes/releases/tag/v1.3.0">going out on July 1, 2016</a>. This was mostly due to the time to resolve a data corruption issue on ungracefully terminated pods caused by detaching of mounted volumes (<a href="https://github.com/kubernetes/kubernetes/issues/27691">#27691</a>). A large number of the bugs introduced in the release were fixed in the 1.3.4 release which <a href="https://github.com/kubernetes/kubernetes/releases/tag/v1.3.4">was cut on August 1, 2016</a>.</li>\n<li><strong>Instability in 1.3\'s Storage stack</strong></li>\n<li>The Kubelet volume redesign shipped in 1.3.0 with several bugs. These were mostly due to unexpected interactions between the new functionality and other Kubernetes components. For example, secrets were handled serially not in parallel, namespace dependencies were not well understood, etc. Most of these issues were quickly identified and addressed but waited for 1.3 patch releases.</li>\n<li>\n<p>Issues related to this include:</p>\n<ul>\n<li>PVC Volume will not detach if PVC or PV is deleted before pod (<a href="https://github.com/kubernetes/kubernetes/issues/29051">#29051</a>)</li>\n<li>GCE PD Detach fails if node no longer exists (<a href="https://github.com/kubernetes/kubernetes/issues/29358">#29358</a>)</li>\n<li>Batch creation of pods that all reference the same secret volume takes a long time (<a href="https://github.com/kubernetes/kubernetes/issues/28616">#28616</a>)</li>\n<li>Error while tearing down pod, "device or resource busy" on service account secret (<a href="https://github.com/kubernetes/kubernetes/issues/28750">#28750</a>)</li>\n</ul>\n</li>\n</ol>\n<h2>Lessons Learned</h2>\n<p>The value of the feature freeze date is to ensure the release has time to stabilize. Refactoring or features that need to be merged past feature freeze date as an exception should be a tool that can be used, albeit sparingly, for the sake of a release. Exceptions should meet certain requirements which the Kubelet Volume Redesign did not meet.</p>\n<h3>Things that went well:</h3>\n<ol>\n<li>The majority of the development went smoothly with adequate testing and little or no instability after release for two of the three sub areas.</li>\n<li>Once the wider impact of the Kubelet Volume Redesign work was understood by the Storage SIG it was widely communicated in the burn down meetings.</li>\n<li>The Storage SIG communication was very tight with hour by hour coordination in this area from beginning to end.</li>\n<li>Many area experts were able to jump in and help debug.</li>\n<li>The community trusted the Storage SIG to make the right decisions.</li>\n</ol>\n<h3>Things that went wrong:</h3>\n<ol>\n<li>The scope of the Kubelet Volume Redesign work and its indirect impact on the system, while understood to be large, was still under-estimated: while the code was written and code reviewed reasonably within schedule, considering the size/extent of the changes, the amount of time required for soaking, stabilization, and testing was insufficient.</li>\n<li>In the decision to move forward with coding beyond code freeze, not enough thought was invested in what could go wrong or how to mitigate that.</li>\n<li>The Storage SIG decisions were not advertised widely enough outside of the SIG early on.</li>\n</ol>\n<h2>Action Items</h2>\n<ol>\n<li>Develop deeper testing across the entire storage stack to allow large changes to be made with confidence.</li>\n<li>\n<p>Status: <a href="https://docs.google.com/document/d/1-u1UA8mBiPZiyYUi7U7Up_e-afVegKmuhmc7fpVQ9hc/edit?ts=57bcd3d4&#x26;pli=1">Planned for 1.5</a></p>\n<ul>\n<li>Discussed at <a href="https://docs.google.com/document/d/1qVL7UE7TtZ_D3P4F7BeRK4mDOvYskUjlULXmRJ4z-oE/edit">Storage-SIG F2F meeting held August 10, 2016</a>. See <a href="https://docs.google.com/document/d/1vA5ul3Wy4GD98x3GZfRYEElfV4OE8dBblSK4rnmrE_M/edit#heading=h.amd7ks7tpscg">notes</a>.</li>\n</ul>\n</li>\n<li>Establish a formal exception process for merging large changes after feature complete dates.</li>\n<li>Status: <a href="https://git.k8s.io/features/EXCEPTIONS.md">Drafted as of 1.4</a></li>\n</ol>\n<p>Kubernetes is an incredibly fast moving project, with hundreds of active contributors creating a solution that thousands of organization rely on. Stability, trust, and openness are paramount in both the product and the community around Kubernetes. We undertook this retrospective effort to learn from the 1.3 release\'s shipping delay. These action items and other work in the upcoming releases are part of our commitment to continually improve our project, our community, and our ability to deliver production-grade infrastructure platform software.</p>'},site:{siteMetadata:{sigs:["sig-api-machinery","sig-apps","sig-architecture","sig-auth","sig-autoscaling","sig-aws","sig-azure","sig-big-data","sig-cli","sig-cloud-provider","sig-cluster-lifecycle","sig-cluster-ops","sig-contributor-experience","sig-docs","sig-gcp","sig-ibmcloud","sig-instrumentation","sig-multicluster","sig-network","sig-node","sig-openstack","sig-product-management","sig-release","sig-scalability","sig-scheduling","sig-service-catalog","sig-storage","sig-testing","sig-ui","sig-vmware","sig-windows"]}}},pathContext:{slug:"/sig-storage/1.3-retrospective/"}}}});
//# sourceMappingURL=path---sig-storage-1-3-retrospective-693939cbfeca3ed95cc7.js.map