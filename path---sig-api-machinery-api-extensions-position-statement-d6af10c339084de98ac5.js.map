{"version":3,"sources":["webpack:///path---sig-api-machinery-api-extensions-position-statement-d6af10c339084de98ac5.js","webpack:///./.cache/json/sig-api-machinery-api-extensions-position-statement.json"],"names":["webpackJsonp","418","module","exports","data","markdownRemark","html","site","siteMetadata","sigs","pathContext","slug"],"mappings":"AAAAA,cAAc,iBAERC,IACA,SAAUC,EAAQC,GCHxBD,EAAAC,SAAkBC,MAAQC,gBAAkBC,KAAA,28NAAu9NC,MAASC,cAAgBC,MAAA,ofAA2fC,aAAgBC,KAAA","file":"path---sig-api-machinery-api-extensions-position-statement-d6af10c339084de98ac5.js","sourcesContent":["webpackJsonp([192668016342287],{\n\n/***/ 418:\n/***/ (function(module, exports) {\n\n\tmodule.exports = {\"data\":{\"markdownRemark\":{\"html\":\"<h1>API Extensions (SIG API Machinery position statement)</h1>\\n<p>Authors: Daniel Smith, David Eads (SIG API Machinery co-leads)<br>\\nLast edit: Feb 23<br>\\nStatus: RELEASED</p>\\n<h2>Background</h2>\\n<p>We have observed a lot of confusion in the community around the general topic\\nof ThirdPartyResources (TPRs) and apiserver aggregation (AA). We want to\\ndocument the current position of the API Machinery SIG.</p>\\n<p>Extremely briefly, TPR is a mechanism for lightweight, easy extension of the\\nkubernetes API, which has collected a <a href=\\\"https://gist.github.com/philips/a97a143546c87b86b870a82a753db14c\\\">significant userbase</a>.\\nAA is a heavier-weight mechanism for accomplishing a similar task; it is\\ntargeted at allowing the Kubernetes project to move away from a monolithic\\napiserver, and as a consequence, it will support PaaSes or other users that\\nneed the complete set of server-side kubernetes API semantics.</p>\\n<h2>Positions</h2>\\n<h3>Q: Do we need two extension mechanisms, or should we provide a single</h3>\\n<p>extension mechanism with multiple opt-in features for users to grow into?\\n(Binary vs gradient)</p>\\n<p>We think there is both room in this space and a necessity for both approaches.\\nTPR is clearly useful to users. In its current state, TPR lacks some features\\nand has some bugs which limit it. We believe TPR bugs should be fixed and some\\nfeatures should be added to it (as long as it maintains its ease-of-use, which\\nwe think is its primary feature). We think TPR’s competitive advantage is its\\nlow barrier-to-entry and ease of use.</p>\\n<p>However, even in the limit where we have added all the features to TPR that\\nmake sense, there’s still a need for apiserver aggregation. Here are two use\\ncases that TPR cannot address while maintaining its ease of use.</p>\\n<ul>\\n<li>Heapster / metrics API. The metrics API is going to be data assembled at read\\ntime, which is extremely high churn and should not be stored in an etcd\\ninstance. Heapster needs to use custom storage.</li>\\n<li>\\n<p>Full-featured extension APIs (pieces of Kubernetes itself; PaaSes).</p>\\n<ul>\\n<li>OpenShift is an example of a full-featured API server that makes use of the\\napimachinery and apiserver features (API versioning, conversion, defaulting,\\nserialization (including protocol buffer encoding), storage, security, custom\\nsubresource handlers, and admission).</li>\\n<li>Integrators who wish to provide this level of features and expect this\\nlevel of API traffic volume are unlikely to be satisfied by webhooks, but\\nshould still be able to integrate. </li>\\n<li>If Kubernetes developers could create new APIs in new apiservers instead\\nof modifying the core apiserver, it would make life better for everyone:</li>\\n<li>Easier to shard reviews</li>\\n<li>Easier to experiment with APIs</li>\\n<li>No more accidentally enabling a half-baked API</li>\\n<li>Code freeze/release train less disruptive</li>\\n<li>It would be great if it were possible to run these extensions (including\\nOpenShift, other PaaSes, and various optional extensions such as the service\\ncatalog) directly on an existing kubernetes cluster; in fact, we think that\\nthe alternative to this is a multiplication of forks, which will be really\\nbad for the ecosystem as a whole. With ecosystem unification in mind, it\\nwould be infeasible to ask any consumer with both many users and an\\nextensive codebase (such as OpenShift) to rewrite their stack in terms of\\nTPRs and webhooks. We have to give such users a path to straight consumption\\nas opposed to the current fork-and-modify approach, which has been the only\\nfeasible one for far too long.</li>\\n</ul>\\n</li>\\n</ul>\\n<p>This is not to say that TPR should stay in its current form. The API Machinery\\nSIG is committed to finishing TPR, making it usable, and maintaining it (but we\\nneed volunteers to step up, or it’s going to take a long time).</p>\\n<p>The big table in <a href=\\\"https://docs.google.com/document/d/1y16jKL2hMjQO0trYBJJSczPAWj8vAgNFrdTZeCincmI/edit#heading=h.xugwibxye5f0\\\">Eric’s comparison doc</a>\\nis a good place to learn the current and possible future feature sets of TPRs\\nand AA. The fact that TPR has been languishing is due to lack of an owner and\\nlack of people willing to work on it, not lack of belief that it ought to be\\nfixed and perfected. Eric and Anirudh have agreed to take on this role.</p>\\n<h3>Q: Should there be a single API object that programs either TPR or AA as appropriate, or should each of these have their own registration object?</h3>\\n<p>We think that the configuration of these two objects is distinct enough that\\ntwo API resources are appropriate.</p>\\n<p>We do need to take care to provide a good user experience, as the API groups\\nusers enter in both AA and TPR come out of the same global namespace. E.g., a\\nuser should not have to make both a TPR registration and an AA registration to\\nstart up a TPR--this would break current users of TPRs.</p>\\n<h3>Q: Should TPRs be fixed up and extended in-place, or should a replacement be built in a separate TPR apiserver?</h3>\\n<p>TPR is implemented currently with a variety of special cases sprinkled\\nthroughout kube-apiserver code. It would greatly simplify kube-apiserver code\\nand the TPR implementation if this were separated, and TPR constructed as its\\nown HTTP server (but still run from kube-apiserver; see bottom Q). However, we\\nwill not block safe, targeted TPR fixes on completion of this split.</p>\\n<h3>Q: Should TPR maintain compatibility, or should we break compatibility to fix and extend it?</h3>\\n<p>There are two dozen open-source projects that use TPR, and we also know of\\nprivate users of TPR, and at least some people consider it to be beta. However,\\nwe may have to implement fixes in a way that requires breaking backward\\ncompatibility. If we do that, we will at a minimum provide migration\\ninstructions and go through a one-release deprecation cycle to give users time\\nto switch over to the new version. We think this decision is probably best made\\nby the people actually working on this (currently: @deads2k, @erictune,\\n@foxish). <a href=\\\"https://docs.google.com/document/d/1Gg158jO1cRBq-8RrWRAWA2IRF9avscuRaWFmY2Wb6qw/edit\\\">Some thoughts here</a>.</p>\\n<h3>Q: Should kube-aggregator be a separate binary/process than kube-apiserver?</h3>\\n<p>For code health reasons, it is very convenient to totally separate the\\naggregation layer from apiserver. However, operationally, it is extremely\\ninconvenient to set up and run an additional binary. Additionally, it is\\ncrucial that all extensibility functionality be in every cluster, because users\\nneed to be able to depend on it; this argues that kube-aggregator can’t be\\noptional.</p>\\n<p>Our current plan is to host several logical apiservers (the existing\\nkube-apiserver, kube-aggregator, and perhaps a hypothetical kube-tprserver,\\nsee above) in a single binary, and launch them in a single process (a drop-in\\nreplacement for the existing kube-apiserver). There are several candidate\\nmechanisms for accomplishing this and we won’t design this in this document. :)</p>\"},\"site\":{\"siteMetadata\":{\"sigs\":[\"sig-api-machinery\",\"sig-apps\",\"sig-architecture\",\"sig-auth\",\"sig-autoscaling\",\"sig-aws\",\"sig-azure\",\"sig-big-data\",\"sig-cli\",\"sig-cloud-provider\",\"sig-cluster-lifecycle\",\"sig-cluster-ops\",\"sig-contributor-experience\",\"sig-docs\",\"sig-gcp\",\"sig-ibmcloud\",\"sig-instrumentation\",\"sig-multicluster\",\"sig-network\",\"sig-node\",\"sig-openstack\",\"sig-product-management\",\"sig-release\",\"sig-scalability\",\"sig-scheduling\",\"sig-service-catalog\",\"sig-storage\",\"sig-testing\",\"sig-ui\",\"sig-vmware\",\"sig-windows\"]}}},\"pathContext\":{\"slug\":\"/sig-api-machinery/api-extensions-position-statement/\"}}\n\n/***/ })\n\n});\n\n\n// WEBPACK FOOTER //\n// path---sig-api-machinery-api-extensions-position-statement-d6af10c339084de98ac5.js","module.exports = {\"data\":{\"markdownRemark\":{\"html\":\"<h1>API Extensions (SIG API Machinery position statement)</h1>\\n<p>Authors: Daniel Smith, David Eads (SIG API Machinery co-leads)<br>\\nLast edit: Feb 23<br>\\nStatus: RELEASED</p>\\n<h2>Background</h2>\\n<p>We have observed a lot of confusion in the community around the general topic\\nof ThirdPartyResources (TPRs) and apiserver aggregation (AA). We want to\\ndocument the current position of the API Machinery SIG.</p>\\n<p>Extremely briefly, TPR is a mechanism for lightweight, easy extension of the\\nkubernetes API, which has collected a <a href=\\\"https://gist.github.com/philips/a97a143546c87b86b870a82a753db14c\\\">significant userbase</a>.\\nAA is a heavier-weight mechanism for accomplishing a similar task; it is\\ntargeted at allowing the Kubernetes project to move away from a monolithic\\napiserver, and as a consequence, it will support PaaSes or other users that\\nneed the complete set of server-side kubernetes API semantics.</p>\\n<h2>Positions</h2>\\n<h3>Q: Do we need two extension mechanisms, or should we provide a single</h3>\\n<p>extension mechanism with multiple opt-in features for users to grow into?\\n(Binary vs gradient)</p>\\n<p>We think there is both room in this space and a necessity for both approaches.\\nTPR is clearly useful to users. In its current state, TPR lacks some features\\nand has some bugs which limit it. We believe TPR bugs should be fixed and some\\nfeatures should be added to it (as long as it maintains its ease-of-use, which\\nwe think is its primary feature). We think TPR’s competitive advantage is its\\nlow barrier-to-entry and ease of use.</p>\\n<p>However, even in the limit where we have added all the features to TPR that\\nmake sense, there’s still a need for apiserver aggregation. Here are two use\\ncases that TPR cannot address while maintaining its ease of use.</p>\\n<ul>\\n<li>Heapster / metrics API. The metrics API is going to be data assembled at read\\ntime, which is extremely high churn and should not be stored in an etcd\\ninstance. Heapster needs to use custom storage.</li>\\n<li>\\n<p>Full-featured extension APIs (pieces of Kubernetes itself; PaaSes).</p>\\n<ul>\\n<li>OpenShift is an example of a full-featured API server that makes use of the\\napimachinery and apiserver features (API versioning, conversion, defaulting,\\nserialization (including protocol buffer encoding), storage, security, custom\\nsubresource handlers, and admission).</li>\\n<li>Integrators who wish to provide this level of features and expect this\\nlevel of API traffic volume are unlikely to be satisfied by webhooks, but\\nshould still be able to integrate. </li>\\n<li>If Kubernetes developers could create new APIs in new apiservers instead\\nof modifying the core apiserver, it would make life better for everyone:</li>\\n<li>Easier to shard reviews</li>\\n<li>Easier to experiment with APIs</li>\\n<li>No more accidentally enabling a half-baked API</li>\\n<li>Code freeze/release train less disruptive</li>\\n<li>It would be great if it were possible to run these extensions (including\\nOpenShift, other PaaSes, and various optional extensions such as the service\\ncatalog) directly on an existing kubernetes cluster; in fact, we think that\\nthe alternative to this is a multiplication of forks, which will be really\\nbad for the ecosystem as a whole. With ecosystem unification in mind, it\\nwould be infeasible to ask any consumer with both many users and an\\nextensive codebase (such as OpenShift) to rewrite their stack in terms of\\nTPRs and webhooks. We have to give such users a path to straight consumption\\nas opposed to the current fork-and-modify approach, which has been the only\\nfeasible one for far too long.</li>\\n</ul>\\n</li>\\n</ul>\\n<p>This is not to say that TPR should stay in its current form. The API Machinery\\nSIG is committed to finishing TPR, making it usable, and maintaining it (but we\\nneed volunteers to step up, or it’s going to take a long time).</p>\\n<p>The big table in <a href=\\\"https://docs.google.com/document/d/1y16jKL2hMjQO0trYBJJSczPAWj8vAgNFrdTZeCincmI/edit#heading=h.xugwibxye5f0\\\">Eric’s comparison doc</a>\\nis a good place to learn the current and possible future feature sets of TPRs\\nand AA. The fact that TPR has been languishing is due to lack of an owner and\\nlack of people willing to work on it, not lack of belief that it ought to be\\nfixed and perfected. Eric and Anirudh have agreed to take on this role.</p>\\n<h3>Q: Should there be a single API object that programs either TPR or AA as appropriate, or should each of these have their own registration object?</h3>\\n<p>We think that the configuration of these two objects is distinct enough that\\ntwo API resources are appropriate.</p>\\n<p>We do need to take care to provide a good user experience, as the API groups\\nusers enter in both AA and TPR come out of the same global namespace. E.g., a\\nuser should not have to make both a TPR registration and an AA registration to\\nstart up a TPR--this would break current users of TPRs.</p>\\n<h3>Q: Should TPRs be fixed up and extended in-place, or should a replacement be built in a separate TPR apiserver?</h3>\\n<p>TPR is implemented currently with a variety of special cases sprinkled\\nthroughout kube-apiserver code. It would greatly simplify kube-apiserver code\\nand the TPR implementation if this were separated, and TPR constructed as its\\nown HTTP server (but still run from kube-apiserver; see bottom Q). However, we\\nwill not block safe, targeted TPR fixes on completion of this split.</p>\\n<h3>Q: Should TPR maintain compatibility, or should we break compatibility to fix and extend it?</h3>\\n<p>There are two dozen open-source projects that use TPR, and we also know of\\nprivate users of TPR, and at least some people consider it to be beta. However,\\nwe may have to implement fixes in a way that requires breaking backward\\ncompatibility. If we do that, we will at a minimum provide migration\\ninstructions and go through a one-release deprecation cycle to give users time\\nto switch over to the new version. We think this decision is probably best made\\nby the people actually working on this (currently: @deads2k, @erictune,\\n@foxish). <a href=\\\"https://docs.google.com/document/d/1Gg158jO1cRBq-8RrWRAWA2IRF9avscuRaWFmY2Wb6qw/edit\\\">Some thoughts here</a>.</p>\\n<h3>Q: Should kube-aggregator be a separate binary/process than kube-apiserver?</h3>\\n<p>For code health reasons, it is very convenient to totally separate the\\naggregation layer from apiserver. However, operationally, it is extremely\\ninconvenient to set up and run an additional binary. Additionally, it is\\ncrucial that all extensibility functionality be in every cluster, because users\\nneed to be able to depend on it; this argues that kube-aggregator can’t be\\noptional.</p>\\n<p>Our current plan is to host several logical apiservers (the existing\\nkube-apiserver, kube-aggregator, and perhaps a hypothetical kube-tprserver,\\nsee above) in a single binary, and launch them in a single process (a drop-in\\nreplacement for the existing kube-apiserver). There are several candidate\\nmechanisms for accomplishing this and we won’t design this in this document. :)</p>\"},\"site\":{\"siteMetadata\":{\"sigs\":[\"sig-api-machinery\",\"sig-apps\",\"sig-architecture\",\"sig-auth\",\"sig-autoscaling\",\"sig-aws\",\"sig-azure\",\"sig-big-data\",\"sig-cli\",\"sig-cloud-provider\",\"sig-cluster-lifecycle\",\"sig-cluster-ops\",\"sig-contributor-experience\",\"sig-docs\",\"sig-gcp\",\"sig-ibmcloud\",\"sig-instrumentation\",\"sig-multicluster\",\"sig-network\",\"sig-node\",\"sig-openstack\",\"sig-product-management\",\"sig-release\",\"sig-scalability\",\"sig-scheduling\",\"sig-service-catalog\",\"sig-storage\",\"sig-testing\",\"sig-ui\",\"sig-vmware\",\"sig-windows\"]}}},\"pathContext\":{\"slug\":\"/sig-api-machinery/api-extensions-position-statement/\"}}\n\n\n//////////////////\n// WEBPACK FOOTER\n// ./~/json-loader!./.cache/json/sig-api-machinery-api-extensions-position-statement.json\n// module id = 418\n// module chunks = 192668016342287"],"sourceRoot":""}