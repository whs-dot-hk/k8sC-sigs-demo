webpackJsonp([51125762411046],{488:function(e,s){e.exports={data:{markdownRemark:{html:'<h2>API call latency SLIs/SLOs details</h2>\n<h3>User stories</h3>\n<ul>\n<li>As a user of vanilla Kubernetes, I want some guarantee how quickly I get the\nresponse from an API call.</li>\n<li>As an administrator of Kubernetes cluster, if I know characteristics of my\nexternal dependencies of apiserver (e.g custom admission plugins, webhooks and\ninitializers) I want to be able to provide guarantees for API calls latency to\nusers of my cluster</li>\n</ul>\n<h3>Other notes</h3>\n<ul>\n<li>We obviously can’t give any guarantee in general, because cluster\nadministrators are allowed to register custom admission plugins, webhooks\nand/or initializers, which we don’t have any control about and they obviously\nimpact API call latencies.</li>\n<li>As a result, we define the SLIs to be very generic (no matter how your\ncluster is set up), but we provide SLO only for default installations (where we\nhave control over what apiserver is doing). This doesn’t provide a false\nimpression, that we provide guarantee no matter how the cluster is setup and\nwhat is installed on top of it.</li>\n<li>At the same time, API calls are part of pretty much every non-trivial workflow\nin Kubernetes, so this metric is a building block for less trivial SLIs and\nSLOs.</li>\n<li>The SLO for latency for read-only API calls of a given type may have significant\nbuffer in threshold. In fact, the latency of the request should be proportional to\nthe amount of work to do (which is number of objects of a given type in a given\nscope) plus some constant overhead. For better tracking of performance, we\nmay want to define purely internal SLI of "latency per object". But that\nisn\'t in near term plans.</li>\n</ul>\n<h3>Caveats</h3>\n<ul>\n<li>The SLO has to be satisfied independently from used encoding in user-originated\nrequests. This makes mix of client important while testing. However, we assume\nthat all <code>core</code> components communicate with apiserver using protocol buffers.</li>\n<li>In case of GET requests, user has an option opt-in for accepting potentially\nstale data (being served from cache) and the SLO again has to be satisfied\nindependently of that. This makes the careful choice of requests in tests\nimportant.</li>\n</ul>\n<h3>TODOs</h3>\n<ul>\n<li>We may consider treating <code>non-namespaced</code> resources as a separate bucket in\nthe future. However, it may not make sense if the number of those may be\ncomparable with <code>namespaced</code> ones.</li>\n</ul>\n<h3>Test scenario</h3>\n<p><strong>TODO: Descibe test scenario.</strong></p>'},site:{siteMetadata:{sigs:["sig-api-machinery","sig-apps","sig-architecture","sig-auth","sig-autoscaling","sig-aws","sig-azure","sig-big-data","sig-cli","sig-cloud-provider","sig-cluster-lifecycle","sig-cluster-ops","sig-contributor-experience","sig-docs","sig-gcp","sig-ibmcloud","sig-instrumentation","sig-multicluster","sig-network","sig-node","sig-openstack","sig-product-management","sig-release","sig-scalability","sig-scheduling","sig-service-catalog","sig-storage","sig-testing","sig-ui","sig-vmware","sig-windows"]}}},pathContext:{slug:"/sig-scalability/slos/api-call-latency/"}}}});
//# sourceMappingURL=path---sig-scalability-slos-api-call-latency-74d40a6a8bb8c63be3a6.js.map